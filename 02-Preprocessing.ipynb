{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download latest version\n",
    "# https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews\n",
    "path = kagglehub.dataset_download(\"snap/amazon-fine-food-reviews\")\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(path + \"/Reviews.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score is the rating of the product. This will be our target variable.\n",
    "df_score = df[\"Score\"]\n",
    "df_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_statistics = df_score.describe()\n",
    "score_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plotting the bar chart for the value counts of the scores\n",
    "score_counts = df_score.value_counts()\n",
    "plt.figure(figsize=(10, 2))\n",
    "score_counts.plot(kind='bar')\n",
    "plt.title('Distribution of Scores')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary is the title of the review. This will be our feature variable.\n",
    "df_summary = df[\"Summary\"]\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rand_idxs = np.random.randint(0, len(df), size=10)\n",
    "for idx in rand_idxs:\n",
    "    print(f\"Score: {df_score.iloc[idx]} - Summary: {df_summary.iloc[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We zero out the data to free up memory\n",
    "df = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Preprocessing steps we will use are:\n",
    "1. Lower Casing\n",
    "2. Replacing URLs\n",
    "3. Replacing Emojis\n",
    "4. Replacing Usernames\n",
    "5. Removing Non-Alphabets\n",
    "6. Removing Consecutive letters\n",
    "7. Removing Short Words\n",
    "8. Removing Stopwords\n",
    "9. Lemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_text(text):\n",
    "    # Convert text to lowercase.\n",
    "    return str(text).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lowercase function to all summaries\n",
    "df_summary = df_summary.apply(lowercase_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a few examples to verify the transformation\n",
    "print(\"After lowercase transformation:\")\n",
    "rand_idxs = np.random.randint(0, len(df_summary), size=10)\n",
    "for idx in rand_idxs:  # Show first 5 examples\n",
    "    print(f\"Score: {df_score.iloc[idx]} - Summary: {df_summary.iloc[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def replace_urls(text):\n",
    "    \"\"\"\n",
    "    Replace URLs in text with a placeholder.\n",
    "    In NLP, we might want to remove URLs to: \n",
    "        Reduce noise - URLs rarely add semantic meaning.\n",
    "        Improve model performance - Less irrelevant data = better learning.\n",
    "        Normalize text - Keeps data consistent and clean.\n",
    "    \"\"\"\n",
    "    # Regular expression pattern to match URLs\n",
    "    # This pattern matches:\n",
    "    # - http:// or https:// at the beginning\n",
    "    # - Followed by any combination of:\n",
    "    #   - Letters (a-z, A-Z)\n",
    "    #   - Numbers (0-9)\n",
    "    #   - Special characters like $-_@.&+\n",
    "    #   - Escaped characters like !*()\n",
    "    #   - URL-encoded characters (e.g., %20 for space)\n",
    "    # The + at the end ensures we match one or more of these characters\n",
    "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    \n",
    "    # Replace URLs with placeholder\n",
    "    return re.sub(url_pattern, 'URL', str(text))\n",
    "\n",
    "# Apply URL replacement to all summaries\n",
    "df_summary = df_summary.apply(replace_urls)\n",
    "\n",
    "# Display a few examples to verify the transformation\n",
    "print(\"After URL replacement:\")\n",
    "rand_idxs = np.random.randint(0, len(df_summary), size=10)\n",
    "for idx in rand_idxs:\n",
    "    print(f\"Score: {df_score.iloc[idx]} - Summary: {df_summary.iloc[idx]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# re.compile will compile the regex pattern into a regex object, necessary for \n",
    "# efficient pattern matching. This creates a reusable pattern object that can be\n",
    "# used multiple times without recompiling the pattern each time, improving performance.\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "\n",
    "    # Emoticons (e.g., üòÄüòÅüòÇü§£üòÉüòÑüòÖüòÜ)\n",
    "    u\"\\U0001F600-\\U0001F64F\"  \n",
    "\n",
    "    # Symbols & pictographs (e.g., üî•üéâüí°üì¶üì±)\n",
    "    u\"\\U0001F300-\\U0001F5FF\"  \n",
    "\n",
    "    # Transport & map symbols (e.g., üöó‚úàÔ∏èüöÄüöâ)\n",
    "    u\"\\U0001F680-\\U0001F6FF\"  \n",
    "\n",
    "    # Flags (e.g., üá∫üá∏üá¨üáßüá®üá¶ ‚Äî these are pairs of regional indicators)\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "\n",
    "    # Dingbats (e.g., ‚úÇÔ∏è‚úàÔ∏è‚úâÔ∏è‚öΩ)\n",
    "    u\"\\u2700-\\u27BF\"          \n",
    "\n",
    "    # Supplemental Symbols & Pictographs (e.g., ü§ñü•∞üß†ü¶æ)\n",
    "    u\"\\U0001F900-\\U0001F9FF\"  \n",
    "\n",
    "    # Symbols & Pictographs Extended-A (e.g., ü™Ñü™Öü™®)\n",
    "    u\"\\U0001FA70-\\U0001FAFF\"  \n",
    "\n",
    "    # Miscellaneous symbols (e.g., ‚òÄÔ∏è‚òÅÔ∏è‚òÇÔ∏è‚ö°)\n",
    "    u\"\\u2600-\\u26FF\"          \n",
    "\n",
    "    \"]+\", flags=re.UNICODE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This pattern will match common text-based emoticons that aren't covered by the emoji Unicode ranges\n",
    "# These emoticons are made up of regular ASCII characters like colons, parentheses, etc.\n",
    "# Examples include:\n",
    "# :) - happy face\n",
    "# :( - sad face\n",
    "# :D - laughing face\n",
    "# ;) - winking face\n",
    "emoticon_pattern = re.compile(r'(:\\)|:\\(|:D|:P|;\\)|:-\\)|:-D|:-P|:\\'\\(|:\\||:\\*)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_and_print(text):\n",
    "    if emoji_pattern.search(text) or emoticon_pattern.search(text):\n",
    "        print(text)\n",
    "        text = emoji_pattern.sub('', text)\n",
    "        text = emoticon_pattern.sub('', text)\n",
    "        print(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = df_summary.apply(remove_and_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing Usernames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def replace_usernames(text):\n",
    "    \"\"\"\n",
    "    Replace usernames (words starting with @) with a generic token.\n",
    "    \n",
    "    Usernames are personally identifiable information and not relevant for sentiment analysis.\n",
    "    Replacing them with a consistent token helps reduce noise and protect privacy.\n",
    "    \"\"\"\n",
    "    # Replace @username with [USER]\n",
    "    return re.sub(r'@\\w+', '[USER]', str(text))\n",
    "\n",
    "# Apply username replacement to all summaries\n",
    "df_summary = df_summary.apply(replace_usernames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Non-Alphabets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_alphabets(text):\n",
    "    \"\"\"\n",
    "    Remove non-alphabetic characters from text.\n",
    "    \n",
    "    Non-alphabetic characters like numbers and special symbols like #, $ etc. often add noise\n",
    "    to text analysis. \n",
    "    Removing them helps focus on the actual words and their meanings,\n",
    "    which is more relevant for sentiment analysis.\n",
    "    \"\"\"\n",
    "    # Keep only alphabetic characters and spaces\n",
    "    return re.sub(r'[^a-zA-Z\\s]', '', str(text))\n",
    "\n",
    "# Apply non-alphabet removal to all summaries\n",
    "df_summary = df_summary.apply(remove_non_alphabets)\n",
    "\n",
    "# Display a sample of the cleaned summaries\n",
    "print(df_summary.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Consecutive letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_consecutive_letters(text):\n",
    "    \"\"\"\n",
    "    Remove consecutive repetitions of letters (more than 3) in text.\n",
    "    \n",
    "    In informal text like reviews, people often express emotions by elongating words\n",
    "    (e.g., 'sooooo good', 'loooove it'). Normalizing these to a maximum of two consecutive\n",
    "    occurrences helps standardize the text while preserving the emphasis.\n",
    "    \"\"\"\n",
    "    # In the following pattern, 3 is the minimum number of consecutive occurrences of the same letter\n",
    "    pattern = r'(\\w)\\1{3,}'\n",
    "    \n",
    "    # Replace with just two occurrences of the letter\n",
    "    return re.sub(pattern, r'\\1\\1', str(text))\n",
    "\n",
    "# Apply consecutive letter removal to all summaries\n",
    "df_summary = df_summary.apply(remove_consecutive_letters)\n",
    "\n",
    "# Display a sample of the cleaned summaries\n",
    "print(df_summary.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Short Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_short_words(text):\n",
    "    \"\"\"\n",
    "    Remove words that are too short (less than 3 characters) from text.\n",
    "    \n",
    "    Very short words like 'a', 'an', 'to', etc. are often not meaningful for sentiment analysis.\n",
    "    Removing these helps reduce noise and focus on more significant words that carry sentiment.\n",
    "    \"\"\"\n",
    "    # Split the text into words\n",
    "    words = str(text).split()\n",
    "    \n",
    "    # Filter out words that are less than 3 characters\n",
    "    filtered_words = [word for word in words if len(word) >= 3]\n",
    "    \n",
    "    # Join the filtered words back into a string\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply short word removal to all summaries\n",
    "df_summary = df_summary.apply(remove_short_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "print(\"Sample stopwords:\", list(stopwords.words('english'))[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Remove common stopwords from text.\n",
    "    \n",
    "    Stopwords are common words like 'the', 'and', 'is', etc. that don't carry much meaning\n",
    "    for sentiment analysis. Removing them helps focus on the more meaningful content words.\n",
    "    \"\"\"\n",
    "    # Get English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Split the text into words\n",
    "    words = str(text).split()\n",
    "    \n",
    "    # Filter out stopwords\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    # Join the filtered words back into a string\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply stopword removal to all summaries\n",
    "df_summary = df_summary.apply(remove_stopwords)\n",
    "\n",
    "# Display a sample of the cleaned summaries\n",
    "print(df_summary.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming and Lemmatization\n",
    "# --------------------------\n",
    "# Both stemming and lemmatization are text normalization techniques used to reduce words to their base forms.\n",
    "#\n",
    "# Stemming:\n",
    "# - A rule-based process that removes word endings to get the root form (stem)\n",
    "# - Fast but often produces non-words (e.g., \"running\" ‚Üí \"run\", but \"trouble\" ‚Üí \"troubl\")\n",
    "# - Less accurate but computationally efficient\n",
    "# - Useful for search engines where exact word forms are less important\n",
    "#\n",
    "# Lemmatization:\n",
    "# - Converts words to their dictionary base form (lemma) using vocabulary and morphological analysis\n",
    "# - More accurate as it produces actual words (e.g., \"better\" ‚Üí \"good\")\n",
    "# - Considers the context and part of speech\n",
    "# - Computationally more intensive but produces better results for NLP tasks\n",
    "# - Preferred for sentiment analysis and other advanced NLP applications\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Download WordNet if not already downloaded\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Example text for demonstration\n",
    "example_text = \"The cats are running and jumping in the fields\"\n",
    "words = example_text.split()\n",
    "\n",
    "print(\"Original words:\", words)\n",
    "\n",
    "# Stemming example\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(\"Stemmed words:\", stemmed_words)\n",
    "\n",
    "# Lemmatizing example\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(\"Lemmatized words:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizing with part of speech tagging\n",
    "# Download necessary resources for POS tagging\n",
    "try:\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "except LookupError:\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Function to lemmatize text with POS tagging\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"\n",
    "    Lemmatize text using WordNet lemmatizer with POS tagging.\n",
    "    \n",
    "    Lemmatization reduces words to their base or dictionary form (lemma) while\n",
    "    considering the part of speech. This is more accurate than stemming as it \n",
    "    produces actual words.\n",
    "    \n",
    "    Example:\n",
    "    - \"running\" -> \"run\" (verb form)\n",
    "    - \"better\" -> \"good\" (adjective form)\n",
    "    \"\"\"\n",
    "    # Tokenize the text\n",
    "    words = str(text).split()\n",
    "    \n",
    "    # Lemmatize each word\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # Join the lemmatized words back into a string\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Apply lemmatization to all summaries\n",
    "df_summary = df_summary.apply(lemmatize_text)\n",
    "\n",
    "# Display a sample of the lemmatized summaries\n",
    "print(\"\\nLemmatized summaries:\")\n",
    "print(df_summary.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Cloud for positive sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Filter summaries for df_score >= 4\n",
    "filtered_summaries = df_summary[df_score >= 4]\n",
    "\n",
    "# Combine all filtered summaries into a single string\n",
    "all_summaries = \" \".join(str(summary) for summary in filtered_summaries)\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_summaries)\n",
    "\n",
    "# Clear the memory\n",
    "all_summaries = 0\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Cloud for negative sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Filter summaries for df_score 1\n",
    "filtered_summaries = df_summary[df_score == 1]\n",
    "\n",
    "# Combine all filtered summaries into a single string\n",
    "all_summaries = \" \".join(str(summary) for summary in filtered_summaries)\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_summaries)\n",
    "\n",
    "# Clear the memory\n",
    "all_summaries = 0\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
