{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download latest version\n",
    "# https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews\n",
    "path = kagglehub.dataset_download(\"snap/amazon-fine-food-reviews\")\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(path + \"/Reviews.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the information of the dataset\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score is the rating of the product. This will be our target variable.\n",
    "df_score = df[\"Score\"]\n",
    "df_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the unique values of the target variable\n",
    "df_score.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the statistics of the target variable\n",
    "score_statistics = df_score.describe()\n",
    "score_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plotting the bar chart for the value counts of the scores\n",
    "score_counts = df_score.value_counts()\n",
    "plt.figure(figsize=(10, 2))\n",
    "score_counts.plot(kind='bar')\n",
    "plt.title('Distribution of Scores')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary is the title of the review. This will constitute our features.\n",
    "df_summary = df[\"Summary\"]\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing numpy for generating random indices\n",
    "import numpy as np\n",
    "# Generating 10 random indices from the range of df_score length\n",
    "rand_idxs = np.random.randint(0, len(df_score), size=10)\n",
    "# Iterating over the random indices to print corresponding Score and Summary\n",
    "for idx in rand_idxs:\n",
    "    print(f\"Score: {df_score.iloc[idx]} - Summary: {df_summary.iloc[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We zero out the data to free up memory\n",
    "df = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Preprocessing steps we will use are:\n",
    "1. Lower Casing\n",
    "2. Replacing URLs\n",
    "3. Replacing Emojis\n",
    "4. Replacing Usernames\n",
    "5. Removing Non-Alphabets\n",
    "6. Removing Consecutive letters\n",
    "7. Removing Short Words\n",
    "8. Removing Stopwords\n",
    "9. Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_text(text):\n",
    "    # Convert text to lowercase.\n",
    "    return str(text).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lowercase function to all summaries\n",
    "df_summary = df_summary.apply(lowercase_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a few examples to verify the transformation\n",
    "print(\"After lowercase transformation:\")\n",
    "rand_idxs = np.random.randint(0, len(df_summary), size=10)\n",
    "for idx in rand_idxs:  \n",
    "    print(f\"Score: {df_score.iloc[idx]} - Summary: {df_summary.iloc[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # Importing the regular expressions module\n",
    "\n",
    "# Define a regex pattern to identify URLs in the text\n",
    "url_pattern = r\"(?:https?|ftp)://[^\\s/$.?#].[^\\s]*\"\n",
    "\n",
    "def replace_urls(text):\n",
    "    \"\"\"\n",
    "    Replace URLs in the text with the token 'URL'.\n",
    "    Prints before and after if a replacement occurs.\n",
    "    \"\"\"\n",
    "    text_str = str(text)\n",
    "    replaced_text = re.sub(url_pattern, 'URL', text_str)\n",
    "\n",
    "    if replaced_text != text_str:\n",
    "        print(f\"Before: {text_str}\")\n",
    "        print(f\"After:  {replaced_text}\\n\")\n",
    "\n",
    "    return replaced_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply URL replacement to all summaries\n",
    "df_summary = df_summary.apply(replace_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# re.compile will compile the regex pattern into a regex object, necessary for \n",
    "# efficient pattern matching. This creates a reusable pattern object that can be\n",
    "# used multiple times without recompiling the pattern each time, improving performance.\n",
    "# u stands for Unicode\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "\n",
    "    # Emoticons (e.g., üòÄüòÅüòÇü§£üòÉüòÑüòÖüòÜ)\n",
    "    u\"\\U0001F600-\\U0001F64F\"  \n",
    "\n",
    "    # Symbols & pictographs (e.g., üî•üéâüí°üì¶üì±)\n",
    "    u\"\\U0001F300-\\U0001F5FF\"  \n",
    "\n",
    "    # Transport & map symbols (e.g., üöó‚úàÔ∏èüöÄüöâ)\n",
    "    u\"\\U0001F680-\\U0001F6FF\"  \n",
    "\n",
    "    # Flags (e.g., üá∫üá∏üá¨üáßüá®üá¶ ‚Äî these are pairs of regional indicators)\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "\n",
    "    # Dingbats (e.g., ‚úÇÔ∏è‚úàÔ∏è‚úâÔ∏è‚öΩ)\n",
    "    u\"\\u2700-\\u27BF\"          \n",
    "\n",
    "    # Supplemental Symbols & Pictographs (e.g., ü§ñü•∞üß†ü¶æ)\n",
    "    u\"\\U0001F900-\\U0001F9FF\"  \n",
    "\n",
    "    # Symbols & Pictographs Extended-A (e.g., ü™Ñü™Öü™®)\n",
    "    u\"\\U0001FA70-\\U0001FAFF\"  \n",
    "\n",
    "    # Miscellaneous symbols (e.g., ‚òÄÔ∏è‚òÅÔ∏è‚òÇÔ∏è‚ö°)\n",
    "    u\"\\u2600-\\u26FF\"          \n",
    "\n",
    "    \"]+\", flags=re.UNICODE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This pattern will match common text-based emoticons that aren't covered by the emoji Unicode ranges\n",
    "# These emoticons are made up of regular ASCII characters like colons, parentheses, etc.\n",
    "# Examples include:\n",
    "# :) - happy face\n",
    "# :( - sad face\n",
    "# :D - laughing face\n",
    "# ;) - winking face\n",
    "emoticon_pattern = re.compile(r'(:\\)|:\\(|:D|:P|;\\)|:-\\)|:-D|:-P|:\\'\\(|:\\||:\\*)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_and_print(text):\n",
    "    if emoji_pattern.search(text) or emoticon_pattern.search(text):\n",
    "        print(f\"Before: {text}\")\n",
    "        text = emoji_pattern.sub('', text)\n",
    "        text = emoticon_pattern.sub('', text)\n",
    "        print(f\"After: {text}\")\n",
    "        print()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = df_summary.apply(remove_and_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing Usernames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def replace_usernames(text):\n",
    "    \"\"\"\n",
    "    Replace email addresses and true @usernames with 'USER'.\n",
    "    Avoid matching embedded @ in profanity or stylized words.\n",
    "    Print before and after if replacement occurs.\n",
    "    \"\"\"\n",
    "    original = str(text)\n",
    "    updated = original\n",
    "\n",
    "    # Replace full email addresses\n",
    "    updated = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', 'USER', updated)\n",
    "\n",
    "    # Replace @usernames only when preceded by space, punctuation, or start of string\n",
    "    updated = re.sub(r'(?:(?<=^)|(?<=[\\s.,;!?]))@\\w+\\b', 'USER', updated)\n",
    "\n",
    "    if updated != original:\n",
    "        print(f\"Before: {original}\")\n",
    "        print(f\"After:  {updated}\\n\")\n",
    "    \n",
    "    return updated\n",
    "\n",
    "\n",
    "# Apply username replacement to all summaries\n",
    "df_summary = df_summary.apply(replace_usernames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Non-Alphabets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text, keep_punct=False):\n",
    "    \"\"\"\n",
    "    Clean and normalize text for NLP classification tasks.\n",
    "    \n",
    "    Parameters:\n",
    "    - text (str): The input text to be cleaned.\n",
    "    - keep_punct (bool): \n",
    "        If True, retains key punctuation (. ! ?) which may carry emotional or contextual weight.\n",
    "        If False, removes all non-alphabetic characters for simpler lexical analysis.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The cleaned text string, lowercased and stripped of unwanted characters.\n",
    "    \n",
    "    This function is designed for flexibility across different NLP tasks like sentiment analysis,\n",
    "    topic classification, or spam detection. It handles:\n",
    "    - Lowercasing text for normalization\n",
    "    - Removing or preserving select punctuation\n",
    "    - Removing digits, symbols, and special characters\n",
    "    - Reducing multiple spaces to a single space\n",
    "    - Optionally printing changes for debugging or logging\n",
    "\n",
    "    When to use `keep_punct=True`:\n",
    "    - Sentiment analysis: punctuation (e.g., \"!\", \"?\") can reflect strong emotion\n",
    "    - Social media or informal text: expressive punctuation often carries signal\n",
    "    - Sarcasm, emphasis, or tone-sensitive tasks\n",
    "\n",
    "    When to use `keep_punct=False`:\n",
    "    - Topic classification or document clustering: punctuation rarely adds value\n",
    "    - Preprocessing for bag-of-words, TF-IDF, or topic modeling\n",
    "    - When punctuation is inconsistent or noisy (e.g., OCR scans, scraped data)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert input to string (safe handling)\n",
    "    original = str(text)\n",
    "\n",
    "    if keep_punct:\n",
    "        # Keep only lowercase letters, spaces, and select punctuation (. ! ?)\n",
    "        # Useful for capturing tone/sentiment\n",
    "        cleaned = re.sub(r\"[^a-z\\s.!?]\", \"\", original)\n",
    "    else:\n",
    "        # Keep only lowercase letters and spaces; remove all punctuation and symbols\n",
    "        cleaned = re.sub(r\"[^a-z\\s]\", \"\", original)\n",
    "\n",
    "    # Normalize whitespace (collapse multiple spaces to one, strip leading/trailing)\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "\n",
    "    # Optional: print before and after if a change occurred\n",
    "    if original != cleaned:\n",
    "        print(f\"Before: {text}\")\n",
    "        print(f\"After:  {cleaned}\\n\")\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply non-alphabet removal to all summaries\n",
    "df_summary = df_summary.apply(lambda x: clean_text(x, keep_punct=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Consecutive letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_consecutive_letters(text, max_repeat=2):\n",
    "    \"\"\"\n",
    "    Normalize elongated words by limiting repeated characters.\n",
    "\n",
    "    In informal or emotional text (e.g., reviews, tweets), users often repeat letters\n",
    "    to add emphasis: \"sooooo good\", \"loooove it\", \"greeaaat\".\n",
    "    \n",
    "    This function reduces any character repeated more than `max_repeat` times \n",
    "    to exactly `max_repeat` occurrences (default: 2), preserving emphasis without bloating vocabulary.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The input text\n",
    "    - max_repeat (int): The maximum allowed repetitions for any character\n",
    "\n",
    "    Returns:\n",
    "    - str: Text with repeated characters normalized\n",
    "    \"\"\"\n",
    "    text_str = str(text)\n",
    "    pattern = r'(\\w)\\1{' + str(max_repeat) + r',}'\n",
    "    cleaned = re.sub(pattern, r'\\1' * max_repeat, text_str)\n",
    "\n",
    "    # Print only if changes were made\n",
    "    if cleaned != text_str:\n",
    "        print(f\"Before: {text_str}\")\n",
    "        print(f\"After:  {cleaned}\\n\")\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply consecutive letter removal to all summaries\n",
    "df_summary = df_summary.apply(lambda x: remove_consecutive_letters(x, max_repeat=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Short Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_short_words(text, min_length=3, preserve_words=None):\n",
    "    \"\"\"\n",
    "    Remove short words from text based on a minimum length threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    - text (str): The input text\n",
    "    - min_length (int): Minimum word length to keep (default = 3)\n",
    "    - preserve_words (set or list): Optional set of short but important words to keep (e.g., {'no', 'not'})\n",
    "    \n",
    "    Returns:\n",
    "    - str: Text with short words removed, except for preserved ones\n",
    "    \n",
    "    Notes:\n",
    "    - Use with care in sentiment analysis. Important short words like 'no', 'not', 'bad' may affect meaning.\n",
    "    - Best used after stopword removal or on very noisy text.\n",
    "    \"\"\"\n",
    "    preserve = set(preserve_words or [])\n",
    "    words = str(text).split()\n",
    "    filtered = [word for word in words if len(word) >= min_length or word.lower() in preserve]\n",
    "    result = ' '.join(filtered)\n",
    "\n",
    "    if result != text:\n",
    "        print(f\"Before: {text}\")\n",
    "        print(f\"After:  {result}\\n\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply short word removal to all summaries\n",
    "df_summary = df_summary.apply(lambda x: remove_short_words(x, min_length=3, preserve_words={'no', 'not'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK (Natural Language Toolkit) is a popular library for natural language processing in Python\n",
    "# https://www.nltk.org/\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "print(\"Sample stopwords:\", list(stopwords.words('english'))[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stopwords but keep critical ones like \"not\"\n",
    "base_stopwords = set(stopwords.words('english'))\n",
    "preserve = {'no', 'not', 'nor', 'never'}\n",
    "custom_stopwords = base_stopwords - preserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Remove stopwords from text, preserving key negation words.\n",
    "\n",
    "    This function uses a customized stopword list that retains important\n",
    "    short words like 'not', 'no', 'nor', and 'never' which carry significant\n",
    "    meaning in tasks like sentiment analysis.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): Lowercased input text\n",
    "\n",
    "    Returns:\n",
    "    - str: Text with stopwords removed, but critical negation words preserved\n",
    "    \"\"\"\n",
    "    words = str(text).split()\n",
    "    filtered = [word for word in words if word not in custom_stopwords]\n",
    "    result = ' '.join(filtered)\n",
    "\n",
    "    if result != text:\n",
    "        print(f\"Before: {text}\")\n",
    "        print(f\"After:  {result}\\n\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply remove_stopwords\n",
    "df_summary = df_summary.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('wordnet')  # Download WordNet, a lexical database of English words\n",
    "nltk.download('omw-1.4')  # WordNet Lemmas sometimes need this, which is a mapping of WordNet lemmas to their Part of Speech (POS) tags.\n",
    "nltk.download('averaged_perceptron_tagger_eng')  # Download English POS tagger\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS mapping function \n",
    "# POS tags can be: ADJ (adjective), ADV (adverb), NOUN (noun), VERB (verb), etc\n",
    "def get_wordnet_pos(tag):\n",
    "    # Determine the WordNet POS tag based on the first letter of the input tag\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ  # Adjective\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB  # Verb\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN  # Noun\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV  # Adverb\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to Noun if no match\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"\n",
    "    Lemmatize text using WordNet lemmatizer with POS tagging.\n",
    "\n",
    "    This version prints each change along with the POS tag of the changed word.\n",
    "    \"\"\"\n",
    "    # Convert the input text to a string to ensure compatibility\n",
    "    original_text = str(text)\n",
    "    # Split the text into individual words\n",
    "    words = original_text.split()\n",
    "    # Obtain Part of Speech (POS) tags for each word\n",
    "    pos_tags = pos_tag(words)\n",
    "\n",
    "    # Initialize lists to store lemmatized words and any changes\n",
    "    lemmatized_words = []\n",
    "    changes = []\n",
    "\n",
    "    # Iterate over each word and its POS tag\n",
    "    for word, tag in pos_tags:\n",
    "        # Map the POS tag to a WordNet POS tag\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        # Lemmatize the word using the mapped POS tag\n",
    "        lemma = lemmatizer.lemmatize(word, wn_tag)\n",
    "\n",
    "        # Check if the lemmatized word is different from the original\n",
    "        if lemma != word:\n",
    "            # Record the change if a difference is found\n",
    "            changes.append((word, lemma, tag))\n",
    "        # Add the lemmatized word to the list\n",
    "        lemmatized_words.append(lemma)\n",
    "\n",
    "    # Join the lemmatized words back into a single string\n",
    "    result = ' '.join(lemmatized_words)\n",
    "\n",
    "    # Print only if there were changes\n",
    "    if changes:\n",
    "        print(f\"\\nOriginal: {original_text}\")\n",
    "        print(f\"Lemmatized: {result}\")\n",
    "        for original, lemma, pos in changes:\n",
    "            print(f\"  - {original} ‚Üí {lemma}  (POS: {pos})\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lemmatization to all summaries\n",
    "df_summary = df_summary.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Cloud for positive sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Filter summaries for df_score >= 4\n",
    "filtered_summaries = df_summary[df_score >= 4]\n",
    "\n",
    "# Combine all filtered summaries into a single string\n",
    "all_summaries = \" \".join(str(summary) for summary in filtered_summaries)\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_summaries)\n",
    "\n",
    "# Clear the memory\n",
    "all_summaries = 0\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Cloud for negative sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Filter summaries for df_score 1\n",
    "filtered_summaries = df_summary[df_score == 1]\n",
    "\n",
    "# Combine all filtered summaries into a single string\n",
    "all_summaries = \" \".join(str(summary) for summary in filtered_summaries)\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_summaries)\n",
    "\n",
    "# Clear the memory\n",
    "all_summaries = 0\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
