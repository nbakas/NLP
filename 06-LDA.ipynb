{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import random\n",
    "# from collections import Counter, defaultdict\n",
    "# import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is downloading the notebook from GitHub and running it\n",
    "import requests\n",
    "from pathlib import Path\n",
    "url = \"https://raw.githubusercontent.com/nbakas/NLP/refs/heads/main/02-Preprocessing.ipynb\"\n",
    "filename = url.split(\"/\")[-1]\n",
    "local_path = Path.cwd() / filename\n",
    "if not local_path.exists():\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    local_path.write_bytes(response.content)\n",
    "%run $local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use gensim library for topic modeling\n",
    "# Import corpora module for document processing\n",
    "from gensim import corpora\n",
    "# Import LdaMulticore for parallel LDA implementation\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "# Import matplotlib for visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert df_summary to list of texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_texts = df_summary.astype(str).tolist()\n",
    "my_texts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_texts = [my_text.split() for my_text in my_texts]\n",
    "processed_texts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a dictionary mapping words to their IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dictionary = corpora.Dictionary(processed_texts)\n",
    "my_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print 10 random items from the dictionary to understand its structure\n",
    "print(\"10 random items from the dictionary:\")\n",
    "import random\n",
    "random_ids = random.sample(list(my_dictionary.keys()), 10)\n",
    "for word_id in random_ids:\n",
    "    print(f\"Word ID {word_id}: {my_dictionary[word_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(my_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter out extreme values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out extreme values (optional)\n",
    "# Filter out extreme values to improve LDA performance and quality\n",
    "# no_below=100: Remove words that appear in fewer than 100 documents (rare terms)\n",
    "#   - Removes noise and very specific terms that don't help identify general topics\n",
    "# no_above=0.1: Remove words that appear in more than 10% of documents (too common)\n",
    "#   - Removes overly common words that appear across many topics and don't help differentiate\n",
    "# This filtering reduces my_dictionary size, speeds up computation, and helps LDA focus on meaningful topic-specific words\n",
    "my_dictionary.filter_extremes(no_below=10, no_above=0.1)\n",
    "my_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(my_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code is creating a \"bag-of-words\" representation of your processed texts using the Gensim library.\n",
    "# In the following line, `my_corpus = [my_dictionary.doc2bow(text) for text in processed_texts]`, each document in `processed_texts` is converted to a bag-of-words format using `doc2bow()`.\n",
    "my_corpus = [my_dictionary.doc2bow(text) for text in processed_texts]\n",
    "my_corpus[:5]\n",
    "# The output `[[(0, 1), (1, 1), (2, 1)], [(3, 1)], [(4, 1)], [(5, 1), (6, 1)], [(7, 1)]]` shows the first 5 documents in your my_corpus:\n",
    "\n",
    "# 1. First document contains words with IDs 0, 1, and 2, each appearing once\n",
    "# 2. Second document contains word with ID 3, appearing once\n",
    "# 3. Third document contains word with ID 4, appearing once\n",
    "# 4. Fourth document contains words with IDs 5 and 6, each appearing once\n",
    "# 5. Fifth document contains word with ID 7, appearing once\n",
    "\n",
    "# Each tuple (word_id, frequency) represents a word by its dictionary ID and how many times it appears in that document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set LDA parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 10  # Number of topics to be extracted\n",
    "my_passes = 10 # Number of my_passes of the corpus through the model during training. More my_passes means better accuracy but longer runtime\n",
    "workers = 4  # Number of worker processes for parallel computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will take ~10 minutes to train the model.\n",
    "# https://radimrehurek.com/gensim/models/ldamulticore.html\n",
    "lda_model = LdaMulticore(\n",
    "    corpus=my_corpus, # The document-term list we created earlier\n",
    "    id2word=my_dictionary, # Maps word IDs to actual words for interpretable output\n",
    "    num_topics=num_topics, # Number of topics to extract \n",
    "    passes=my_passes, # Number of training my_passes through the corpus \n",
    "    workers=workers, # Number of parallel processes to use \n",
    "    alpha='symmetric', # Topic distribution prior - 'symmetric' gives equal probability to all topics initially\n",
    "    eta='auto' # Word distribution prior (influences how words are distributed across topics). 'auto' lets the model learn optimal word weights. β in notes.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate LDA model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coherence score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coherence score - measures the semantic similarity between high scoring words in topics. Calculate coherence score using c_v (coherence of vectors) measure (based on sliding window, normalized pointwise mutual information and cosine similarity). Coherence score is a measure of how well the topics are defined by the words in the topics. It takes into account the semantic similarity between the words in the topics. It takes values between 0 and 1, with 1 being the highest coherence. Typical values are between 0.3 and 0.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://radimrehurek.com/gensim/models/coherencemodel.html\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_texts, dictionary=my_dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model_lda.get_coherence()\n",
    "print(f\"Coherence Score: {coherence_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity - a measure of how well the model predicts a sample (lower is better). \n",
    "- It's the exponential of the negative average log-likelihood per word\n",
    "- Typical perplexity values for LDA models are usually in the range of 100–1000\n",
    "- Lower values (e.g., < 100) indicate better generalization (less surprise),\n",
    "- but very low perplexity on the training set (e.g., < 50) can be a sign of overfitting,\n",
    "meaning the model fits the training data too closely and may not generalize well to unseen data\n",
    "- Very high values (e.g., > 1000) suggest poor topic modeling or an inappropriate number of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perplexity Formula:**\n",
    "\n",
    "If `log_perplexity` is the negative average log-likelihood per word (from Gensim):\n",
    "\n",
    "Perplexity = e^(-log_perplexity)\n",
    "\n",
    "Where:\n",
    "- `log_perplexity` is returned by `lda_model.log_perplexity(corpus)`\n",
    "- `exp` is the exponential function (base *e*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| log_perplexity | Actual perplexity | Interpretation                  |\n",
    "|----------------|-------------------|---------------------------------|\n",
    "| -5             | ~148              | Very good fit                   |\n",
    "| -6             | ~403              | Good                            |\n",
    "| -7             | ~1097             | Acceptable to borderline high   |\n",
    "| -8             | ~2980             | Likely too high → poor generalization |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = lda_model.log_perplexity(my_corpus)\n",
    "print(f\"Perplexity: {perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic-term weight statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# The topic-term matrix represents the distribution of terms across topics\n",
    "topic_term_matrix = lda_model.get_topics()\n",
    "print(topic_term_matrix.shape)\n",
    "topic_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the terms in the topic-term matrix\n",
    "for topic_idx, topic in enumerate(topic_term_matrix):\n",
    "    print(f\"Topic {topic_idx + 1}:\")\n",
    "    top_terms_indices = topic.argsort()[-10:][::-1]  # Get indices of top 10 terms\n",
    "    top_terms = [lda_model.id2word[idx] for idx in top_terms_indices]  # Map indices to terms\n",
    "    top_terms_probs = [topic[idx] for idx in top_terms_indices]  # Get probabilities of top 10 terms\n",
    "    top_terms_with_probs = [f\"{term} ({prob:.4f})\" for term, prob in zip(top_terms, top_terms_probs)]\n",
    "    print(\"Top terms:\", \", \".join(top_terms_with_probs))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 30\n",
    "top_words = set()\n",
    "\n",
    "# Collect top-N words across all topics\n",
    "for topic in lda_model.get_topics():\n",
    "    top_indices = topic.argsort()[-top_n:][::-1]\n",
    "    top_words.update(top_indices)\n",
    "\n",
    "# Convert to sorted list\n",
    "top_words = sorted(list(top_words))\n",
    "\n",
    "# Slice topic_term_matrix to include only top-N words\n",
    "reduced_topic_matrix = topic_term_matrix[:, top_words]\n",
    "\n",
    "# Now compute distinctiveness as before\n",
    "topic_similarities = np.zeros((num_topics, num_topics))\n",
    "for i in range(num_topics):\n",
    "    for j in range(num_topics):\n",
    "        if i != j:\n",
    "            similarity = np.dot(reduced_topic_matrix[i], reduced_topic_matrix[j]) / (\n",
    "                np.linalg.norm(reduced_topic_matrix[i]) * np.linalg.norm(reduced_topic_matrix[j])\n",
    "            )\n",
    "            topic_similarities[i, j] = similarity\n",
    "\n",
    "avg_similarities = np.mean(topic_similarities, axis=1)\n",
    "topic_distinctiveness = 1 - avg_similarities\n",
    "\n",
    "print(\"\\nTopic Distinctiveness (higher is better):\")\n",
    "for i, d in enumerate(topic_distinctiveness):\n",
    "    print(f\"Topic {i+1}: {d:.4f}\")\n",
    "print(f\"Average Topic Distinctiveness: {np.mean(topic_distinctiveness):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the topics\n",
    "print(\"\\nGensim LDA Topics:\")\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"Topic {idx+1}: {topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize LDA topics using pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "# Prepare the visualization\n",
    "vis_data = gensimvis.prepare(lda_model, my_corpus, my_dictionary)\n",
    "\n",
    "# Set the figure size for better visualization\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# Display the interactive visualization\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 10 random documents and print their topics\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Select 10 random document indices\n",
    "random_doc_indices = random.sample(range(len(my_corpus)), 10)\n",
    "\n",
    "print(\"\\nTopic Distribution for 10 Random Documents:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for idx in random_doc_indices:\n",
    "    # Get the document's topic distribution\n",
    "    doc_topics = lda_model.get_document_topics(my_corpus[idx])\n",
    "    \n",
    "    # Sort topics by probability (highest first)\n",
    "    doc_topics = sorted(doc_topics, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get the original text (if available)\n",
    "    original_text = df_summary.iloc[idx]\n",
    "    \n",
    "    print(f\"\\nDocument {idx}: \\\"{original_text}\\\"\")\n",
    "    print(\"Topic Distribution:\")\n",
    "    \n",
    "    for topic_id, prob in doc_topics[:3]:\n",
    "        # Get the top words for this topic\n",
    "        topic_words = lda_model.show_topic(topic_id, topn=5)\n",
    "        words = \", \".join([word for word, _ in topic_words])\n",
    "        \n",
    "        # Format the probability as a percentage\n",
    "        prob_percent = prob * 100\n",
    "        \n",
    "        print(f\"  Topic {topic_id+1}: {prob_percent:.2f}% ({words})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
