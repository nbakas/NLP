{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is downloading the notebook from GitHub and running it\n",
    "import requests\n",
    "from pathlib import Path\n",
    "url = \"https://raw.githubusercontent.com/nbakas/NLP/refs/heads/main/02-Preprocessing.ipynb\"\n",
    "filename = url.split(\"/\")[-1]\n",
    "local_path = Path.cwd() / filename\n",
    "if not local_path.exists():\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    local_path.write_bytes(response.content)\n",
    "%run {str(local_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use gensim library for topic modeling\n",
    "# Import corpora module for document processing\n",
    "from gensim import corpora\n",
    "# Import LdaMulticore for parallel LDA implementation\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "# Import matplotlib for visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert df_summary to list of texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_texts = df_summary.astype(str).tolist()\n",
    "my_texts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_texts = [my_text.split() for my_text in my_texts]\n",
    "processed_texts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a dictionary mapping words to their IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is creating a dictionary of words and their IDs from the processed texts.\n",
    "my_dictionary = corpora.Dictionary(processed_texts)\n",
    "my_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print 10 random items from the dictionary to understand its structure\n",
    "print(\"10 random items from the dictionary:\")\n",
    "import random\n",
    "random_ids = random.sample(list(my_dictionary.keys()), 10)\n",
    "for word_id in random_ids:\n",
    "    print(f\"Word ID {word_id}: {my_dictionary[word_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(my_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter out extreme values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out extreme values (optional) to improve LDA performance and quality\n",
    "# no_below=100: Remove words that appear in fewer than 100 documents (rare terms)\n",
    "#   - Removes noise and very specific terms that don't help identify general topics\n",
    "# no_above=0.1: Remove words that appear in more than 10% of documents (too common)\n",
    "#   - Removes overly common words that appear across many topics and don't help differentiate\n",
    "# This filtering reduces my_dictionary size, speeds up computation, and helps LDA focus on meaningful topic-specific words\n",
    "my_dictionary.filter_extremes(no_below=10, no_above=0.1)\n",
    "my_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(my_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is creating a \"bag-of-words\" representation of your processed texts using the Gensim library.\n",
    "# In the following line, `my_corpus = [my_dictionary.doc2bow(text) for text in processed_texts]`, each document in `processed_texts` is converted to a bag-of-words format using `doc2bow()`.\n",
    "my_corpus = [my_dictionary.doc2bow(text) for text in processed_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_corpus[:10]\n",
    "# Each tuple (word_id, frequency) represents a word by its dictionary ID and how many times it appears in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 10 documents in the corpus, their words and their IDs\n",
    "for doc_id, doc in enumerate(my_corpus[:10]):\n",
    "    print(f\"Document {doc_id+1}:\")\n",
    "    for word_id, freq in doc:\n",
    "        word = my_dictionary[word_id]\n",
    "        print(f\"  Word ID {word_id} ('{word}'): Frequency {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Note on Test Set Evaluation**\n",
    "\n",
    "Since **LDA is an unsupervised learning method**, we often use the full dataset to build the model — there are no labels to predict, as in supervised learning.\n",
    "\n",
    "However, using a **test set** can still be very useful.\n",
    "It helps:\n",
    "\n",
    "* Evaluate how well the model **generalizes to new, unseen data**\n",
    "* Compare models with different **numbers of topics**\n",
    "* Avoid **overfitting** to the training data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set LDA parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 10  # Number of topics to be extracted\n",
    "my_passes = 10 # Number of my_passes of the corpus through the model during training. More my_passes means better accuracy but longer runtime\n",
    "workers = 4  # Number of worker processes for parallel computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will take ~10 minutes to train the model if the dictionary is not filtered.\n",
    "# https://radimrehurek.com/gensim/models/ldamulticore.html\n",
    "lda_model = LdaMulticore(\n",
    "    corpus=my_corpus, # The document-term list we created earlier\n",
    "    id2word=my_dictionary, # Maps word IDs to actual words for interpretable output\n",
    "    num_topics=num_topics, # Number of topics to extract \n",
    "    passes=my_passes, # Number of training my_passes through the corpus \n",
    "    workers=workers, # Number of parallel processes to use \n",
    "    alpha='symmetric', # Topic distribution prior - 'symmetric' gives equal probability to all topics initially\n",
    "    eta='auto' # Word distribution prior (influences how words are distributed across topics). 'auto' lets the model learn optimal word weights. β in notes.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate LDA model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coherence score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A coherence score tells us if the words in a topic appear in similar texts or contexts.\n",
    "- For example, in a good topic like: [\"dog\", \"cat\", \"pet\", \"animal\"], these words often show up in the same documents.\n",
    "- It takes values between 0 and 1, with 1 being the highest coherence. Typical values are between 0.3 and 0.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://radimrehurek.com/gensim/models/coherencemodel.html\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, # LDA model\n",
    "                                     texts=processed_texts, # list of texts, each text is a list of words\n",
    "                                     dictionary=my_dictionary, # dictionary of words and their IDs\n",
    "                                     coherence='c_v', # coherence measure, c_v id defined as the average pointwise mutual information of all word pairs in a topic\n",
    "                                     topn=20 # number of words to consider for coherence score\n",
    "                                     )\n",
    "coherence_score = coherence_model_lda.get_coherence()\n",
    "print(f\"Coherence Score: {coherence_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perplexity** is a measure of how well a topic model can **explain the words** in a new document.\n",
    "\n",
    "* It uses the **words in the document** to guess which topics are present.\n",
    "* Then it checks how **likely those words** are, based on the **learned topic-word probabilities**.\n",
    "* It does **not care about word order** — only which words appear and how often.\n",
    "\n",
    "**Lower perplexity = better fit**\n",
    "(The model is less “surprised” by the document’s words.)\n",
    "\n",
    "Even if the document has **multiple topics**, the perplexity can still be low — as long as the topics match well with the document’s words.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perplexity is the exponential of the negative average log-likelihood per word\n",
    "- Typical perplexity values for LDA models are usually in the range of 100–1000\n",
    "- Lower values (e.g., < 100) indicate better generalization (less surprise),\n",
    "- However, very low perplexity on the training set (e.g., < 50) can be a sign of overfitting,\n",
    "meaning the model fits the training data too closely and may not generalize well to unseen data\n",
    "- Very high values (e.g., > 1000) suggest poor topic modeling or an inappropriate number of topics\n",
    "  \n",
    "**Perplexity Formula:**\n",
    "\n",
    "If `log_perplexity` is the negative average log-likelihood per word (from Gensim):\n",
    "\n",
    "Perplexity = e^(-log_perplexity)\n",
    "\n",
    "Where:\n",
    "- `log_perplexity` is returned by `lda_model.log_perplexity(corpus)`\n",
    "- `exp` is the exponential function (base *e*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| log_perplexity | Actual perplexity | Interpretation                  |\n",
    "|----------------|-------------------|---------------------------------|\n",
    "| -5             | ~148              | Very good fit                   |\n",
    "| -6             | ~403              | Good                            |\n",
    "| -7             | ~1097             | Acceptable to borderline high   |\n",
    "| -8             | ~2980             | Likely too high → poor generalization |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = lda_model.log_perplexity(my_corpus)\n",
    "print(f\"Perplexity: {perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize LDA topics using pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "# Prepare the visualization\n",
    "vis_data = gensimvis.prepare(lda_model, my_corpus, my_dictionary)\n",
    "\n",
    "# Set the figure size for better visualization\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# Display the interactive visualization\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 10 random documents and print their topics\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Select 10 random document indices\n",
    "random_doc_indices = random.sample(range(len(my_corpus)), 10)\n",
    "\n",
    "print(\"\\nTopic Distribution for 10 Random Documents:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for idx in random_doc_indices:\n",
    "    # Get the document's topic distribution\n",
    "    doc_topics = lda_model.get_document_topics(my_corpus[idx])\n",
    "    \n",
    "    # Sort topics by probability (highest first)\n",
    "    doc_topics = sorted(doc_topics, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get the original text (if available)\n",
    "    original_text = df_summary.iloc[idx]\n",
    "    \n",
    "    print(f\"\\nDocument {idx}: \\\"{original_text}\\\"\")\n",
    "    print(\"Topic Distribution:\")\n",
    "    \n",
    "    for topic_id, prob in doc_topics[:3]:\n",
    "        # Get the top words for this topic\n",
    "        topic_words = lda_model.show_topic(topic_id, topn=5)\n",
    "        words = \", \".join([word for word, _ in topic_words])\n",
    "        \n",
    "        # Format the probability as a percentage\n",
    "        prob_percent = prob * 100\n",
    "        \n",
    "        print(f\"  Topic {topic_id+1}: {prob_percent:.2f}% ({words})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
