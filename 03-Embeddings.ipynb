{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you get an error when loading Word2Vec, install scipy==1.12\n",
    "# !pip install scipy==1.12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is focused on demonstrating various text representation techniques used in Natural Language Processing. It starts by defining a simple text corpus with sentences about different text representation methods like OneHot vectors, Bag of Words, TF-IDF, N-grams, and Word Embeddings. The corpus is then split into sentences and further into words, with all text converted to lowercase. A vocabulary of unique words is created, and each word is represented as a binary vector using one-hot encoding. The Bag of Words model counts the frequency of each word in the corpus using CountVectorizer from sklearn. TF-IDF is applied to the corpus using TfidfVectorizer from sklearn, which scales word frequencies by their importance across documents. N-grams are generated using CountVectorizer with an n-gram range of two, capturing pairs of consecutive words in the corpus. Finally it presents the Transformers library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_corpus = f\"\"\"\n",
    "    OneHot vectors are binary vectors.\n",
    "    \n",
    "    Bag of Words Counts words.\n",
    "    \n",
    "    TFIDF Counts words and weights words by importance.\n",
    "    \n",
    "    Ngrams Captures words sequences.\n",
    "    \n",
    "    Words Embeddings with Dense vectors.\n",
    "    \"\"\"\n",
    "my_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the corpus into sentences by using '.' as a delimiter and remove the last empty element\n",
    "my_corpus = my_corpus.split('.')[:-1]\n",
    "# Strip leading and trailing whitespace from each sentence and filter out any empty sentences\n",
    "my_corpus = [sentence.strip() for sentence in my_corpus if sentence]\n",
    "my_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the corpus\n",
    "tokenized_corpus = [doc.lower().split() for doc in my_corpus]\n",
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique words\n",
    "# Extract all unique words from the tokenized corpus and sort them\n",
    "all_words = sorted(list(set(word for doc in tokenized_corpus for word in doc)))\n",
    "# Print the size of the vocabulary\n",
    "print(f\"Vocabulary size: {len(all_words)}\")\n",
    "# Print the list of all unique words\n",
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create an identity matrix where each row represents a one-hot encoded vector for each unique word\n",
    "one_hot_word_vectors = np.eye(len(all_words))\n",
    "\n",
    "# Convert the one-hot encoded vectors into a DataFrame for better readability, using unique words as column headers\n",
    "one_hot_word_vectors_df = pd.DataFrame(one_hot_word_vectors, columns=all_words)\n",
    "\n",
    "# Display the DataFrame containing one-hot encoded vectors\n",
    "one_hot_word_vectors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a zero matrix to store one-hot encoded vectors for each *document* in the corpus\n",
    "corpus_vectors = np.zeros((len(tokenized_corpus), len(all_words)))\n",
    "\n",
    "# Iterate over each document and its index in the tokenized corpus\n",
    "for i, doc in enumerate(tokenized_corpus):\n",
    "    # Iterate over each word in the document\n",
    "    for word in doc:\n",
    "        # Iterate over each word and its index in the list of all unique words\n",
    "        for j, w in enumerate(all_words):\n",
    "            # If the word matches the unique word, set the corresponding position in the matrix to 1\n",
    "            if w == word:\n",
    "                corpus_vectors[i, j] = 1\n",
    "\n",
    "# Convert the matrix of one-hot encoded vectors into a DataFrame for better readability\n",
    "corpus_vectors_df = pd.DataFrame(corpus_vectors, columns=all_words)\n",
    "\n",
    "# Display the DataFrame containing one-hot encoded vectors for the corpus\n",
    "corpus_vectors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_vectors_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Bag of Words representation \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the corpus\n",
    "bow_matrix = count_vectorizer.fit_transform(my_corpus)\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "bow_df = pd.DataFrame(\n",
    "    bow_matrix.toarray(),\n",
    "    columns=count_vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "# Display the Bag of Words representation\n",
    "print(\"Bag of Words representation:\")\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the corpus\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(my_corpus)\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(),\n",
    "    columns=tfidf_vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "# Display the TF-IDF representation\n",
    "print(\"TF-IDF representation:\")\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the CountVectorizer with n-gram range\n",
    "# ngram_range : is a tuple of two integers (min_n, max_n)\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "# Fit and transform the corpus\n",
    "ngram_matrix = ngram_vectorizer.fit_transform(my_corpus)\n",
    "ngram_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for better visualization\n",
    "ngram_df = pd.DataFrame(\n",
    "    ngram_matrix.toarray(),\n",
    "    columns=ngram_vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "# Display the N-grams representation\n",
    "print(\"N-grams shape:\", ngram_matrix.shape)\n",
    "ngram_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Word Embeddings (Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Initialize the Word2Vec model\n",
    "word2vec_model = Word2Vec(tokenized_corpus, # The corpus to train the model on\n",
    "                          vector_size=100, # The dimensionality of the vectors\n",
    "                          window=5, # The window size for the context window\n",
    "                          epochs=5, # The number of epochs to train the model\n",
    "                          min_count=1, # The minimum number of times a word must appear in the corpus to be included in the model\n",
    "                          workers=4) # The number of threads to use for training\n",
    "\n",
    "# Get the word embeddings\n",
    "word_embeddings = word2vec_model.wv\n",
    "\n",
    "# Print the word vectors\n",
    "print(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the dimensionality of the vectors\n",
    "print(\"\\nVector dimensionality:\", word_embeddings.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the vector for a specific word\n",
    "my_word = \"words\"\n",
    "print(f\"\\nVector for '{my_word}':\")\n",
    "print(word_embeddings[my_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also find similar words\n",
    "print(f\"\\nWords similar to '{my_word}':\")\n",
    "similar_words = word_embeddings.most_similar(my_word, topn=len(word_embeddings.index_to_key))\n",
    "for idx, (word, similarity) in enumerate(similar_words):\n",
    "    print(f\"{idx+1}. {word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_embeddings.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotly heatmap\n",
    "import plotly.express as px\n",
    "def visualize_similarity_matrix(similarity_df):\n",
    "    fig = px.imshow(similarity_df, labels=dict(x=\"Words\", y=\"Words\", color=\"Similarity\"), x=similarity_df.columns, y=similarity_df.index, color_continuous_scale=\"Viridis\")\n",
    "    fig.update_layout(title=\"Word Similarity Matrix\", xaxis_tickangle=-45, width=800, height=800)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity matrix\n",
    "# Create a similarity matrix manually since KeyedVectors doesn't have similarity_matrix attribute\n",
    "import numpy as np\n",
    "words = word_embeddings.index_to_key\n",
    "similarity_matrix = np.zeros((len(words), len(words)))\n",
    "\n",
    "for i, word1 in enumerate(words):\n",
    "    for j, word2 in enumerate(words):\n",
    "        if word1 != word2:\n",
    "            similarity_matrix[i, j] = word_embeddings.similarity(word1, word2)\n",
    "\n",
    "# Create a DataFrame for the similarity matrix\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=words, columns=words)\n",
    "visualize_similarity_matrix(similarity_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_2d_plot(df):\n",
    "    # Create a scatter plot using Plotly\n",
    "    fig = px.scatter(df, x='C1', y='C2', text='doc', title='Visualization of Word Embeddings', labels=[\"Component 1\", \"Component 2\"])\n",
    "\n",
    "    # Improve the layout\n",
    "    fig.update_traces(textposition='top center', marker=dict(size=10, opacity=0.8), mode='markers+text')\n",
    "    fig.update_layout(width=900, height=700, xaxis=dict(title='Component 1'), yaxis=dict(title='Component 2'))\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA plot\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Initialize PCA with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit PCA on the word embeddings\n",
    "pca.fit(word_embeddings.vectors)\n",
    "\n",
    "# Transform the word embeddings using PCA\n",
    "word_embeddings_2d = pca.transform(word_embeddings.vectors)\n",
    "\n",
    "# Create a DataFrame for the 2D embeddings\n",
    "pca_df = pd.DataFrame(\n",
    "    word_embeddings_2d,\n",
    "    columns=['C1', 'C2']\n",
    ")\n",
    "pca_df['doc'] = words\n",
    "visualize_2d_plot(pca_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSNE plot\n",
    "# t-SNE tries to preserve local relationships, not the global structure. For a small number of points (e.g., ~20 words), t-SNE often:\n",
    "# Overemphasizes tiny distances\n",
    "# Distorts distances between points not in a neighborhood\n",
    "# Gives unpredictable layouts that \"feel random\"\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Initialize TSNE with 2 components\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "\n",
    "# Fit and transform the word embeddings\n",
    "# Set perplexity to a value less than the number of samples\n",
    "# The perplexity is the number of samples in a neighborhood of a selected point\n",
    "# Default perplexity is 30, so we need to reduce it if we have fewer than 30 samples\n",
    "n_samples = word_embeddings.vectors.shape[0]\n",
    "perplexity = min(30, n_samples - 1)  # Ensure perplexity is less than n_samples\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity, init='pca')\n",
    "word_embeddings_2d = tsne.fit_transform(word_embeddings.vectors)\n",
    "\n",
    "# Create a DataFrame for the 2D embeddings\n",
    "tsne_df = pd.DataFrame(\n",
    "    word_embeddings_2d,\n",
    "    columns=['C1', 'C2']\n",
    ")\n",
    "tsne_df['doc'] = words\n",
    "\n",
    "visualize_2d_plot(tsne_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the embeddings of the sentences in the corpus from the word embeddings\n",
    "# Initialize an empty list to store sentence embeddings\n",
    "sentence_embeddings = np.zeros((len(tokenized_corpus), word_embeddings.vector_size))\n",
    "# Iterate through each document in the corpus\n",
    "for i, doc in enumerate(tokenized_corpus):\n",
    "    # Initialize a numpy array of zeros for the sentence vector\n",
    "    sentence_vector = np.zeros(word_embeddings.vector_size)\n",
    "    word_count = 0\n",
    "    \n",
    "    # Iterate through each word and add its vector to the sentence vector\n",
    "    for word in doc:\n",
    "        if word in word_embeddings:\n",
    "            sentence_vector += word_embeddings[word]\n",
    "            word_count += 1\n",
    "    \n",
    "    # If we found words in the model, calculate the average\n",
    "    if word_count > 0:\n",
    "        sentence_vector = sentence_vector / word_count\n",
    "    \n",
    "    # Add the sentence embedding to our list\n",
    "    sentence_embeddings[i] = sentence_vector\n",
    "# Create a DataFrame with the sentence embeddings\n",
    "# The error occurs because word_embeddings.index_to_key has 17 items but our vectors have 100 dimensions\n",
    "# We need to create column names that match the dimensions of our vectors\n",
    "sentence_embeddings_df = pd.DataFrame(\n",
    "    sentence_embeddings,\n",
    "    columns=[f\"dim_{i}\" for i in range(word_embeddings.vector_size)]\n",
    ")\n",
    "sentence_embeddings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity matrix for document embeddings\n",
    "# Create a similarity matrix manually since KeyedVectors doesn't have similarity_matrix attribute\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity_matrix = np.zeros((len(my_corpus), len(my_corpus)))\n",
    "for i, embedding_i in enumerate(sentence_embeddings):\n",
    "    for j, embedding_j in enumerate(sentence_embeddings):\n",
    "        if i != j:\n",
    "            similarity_matrix[i, j] = cosine_similarity(embedding_i.reshape(1, -1), embedding_j.reshape(1, -1))[0, 0]\n",
    "\n",
    "# Create a DataFrame for the similarity matrix\n",
    "doc_names = [\"doc_\" + str(i+1) for i in range(len(tokenized_corpus))]\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=doc_names, columns=doc_names)\n",
    "visualize_similarity_matrix(similarity_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PCA for document embeddings\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit PCA on the word embeddings\n",
    "pca.fit(sentence_embeddings)\n",
    "\n",
    "# Transform the word embeddings using PCA\n",
    "sentence_embeddings_2d = pca.transform(sentence_embeddings)\n",
    "\n",
    "# Create a DataFrame for the 2D embeddings\n",
    "pca_df = pd.DataFrame(\n",
    "    sentence_embeddings_2d,\n",
    "    columns=['C1', 'C2']\n",
    ")\n",
    "pca_df['doc'] = [\"doc_\" + str(i) for i in range(len(tokenized_corpus))]\n",
    "visualize_2d_plot(pca_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environmental Variables\n",
    "we will need to use Environment Variables:\n",
    "- HF_TOKEN is you huggingface token, you may generate one on this url: https://huggingface.co/settings/tokens\n",
    "\n",
    "## On Linux do:\n",
    "- `nano ~/.bashrc`\n",
    "- `export HF_TOKEN=\"...\"`\n",
    "- `source ~/.bashrc`\n",
    "- `echo $HF_TOKEN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HF_TOKEN'] = \"your huggingface token here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_HOME\"] = r\"C:\\my_hf_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/sentence-transformers\n",
    "# !pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/get-started/locally/\n",
    "# !pip3 install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "if torch.cuda.device_count()>0:\n",
    "    my_device = \"cuda\"\n",
    "    print(f\"You have {torch.cuda.device_count()} GPUs available.\")\n",
    "else:\n",
    "    my_device = \"cpu\"\n",
    "    print(\"You have no GPUs available. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "embeddings_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2',\n",
    "                                       token=os.environ[\"HF_TOKEN\"],\n",
    "                                       cache_folder=os.environ[\"HF_HOME\"],\n",
    "                                       device=my_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the corpus using the embeddings model\n",
    "word_embeddings_transformer = embeddings_model.encode(my_corpus)\n",
    "\n",
    "# Print the shape of the resulting embeddings\n",
    "print(word_embeddings_transformer.shape)\n",
    "\n",
    "# Output the embeddings\n",
    "word_embeddings_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a zero matrix to store similarity scores between documents\n",
    "similarity_matrix = np.zeros((len(my_corpus), len(my_corpus)))\n",
    "\n",
    "# Iterate over each pair of embeddings to compute cosine similarity\n",
    "for i, embedding_i in enumerate(word_embeddings_transformer):\n",
    "    for j, embedding_j in enumerate(word_embeddings_transformer):\n",
    "        # Avoid computing similarity of a document with itself\n",
    "        if i != j:\n",
    "            # Compute and store the cosine similarity between different document embeddings\n",
    "            similarity_matrix[i, j] = cosine_similarity(embedding_i.reshape(1, -1), embedding_j.reshape(1, -1))[0, 0]\n",
    "\n",
    "# Create a DataFrame for the similarity matrix\n",
    "doc_names = [\"doc_\" + str(i+1) for i in range(len(tokenized_corpus))]\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=doc_names, columns=doc_names)\n",
    "visualize_similarity_matrix(similarity_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Embeddings - related words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of words to analyze\n",
    "word_list = [\"book\", \"book!\", \"publication\", \"article\"]\n",
    "\n",
    "# Encode the list of words using the embeddings model\n",
    "word_embeddings_transformer = embeddings_model.encode(word_list)\n",
    "\n",
    "# Calculate the cosine similarity matrix for the encoded words\n",
    "cosine_similarities = cosine_similarity(word_embeddings_transformer)\n",
    "\n",
    "# Print the cosine similarity matrix\n",
    "print(\"Cosine Similarity Matrix:\")\n",
    "print(cosine_similarities)\n",
    "\n",
    "# Create a DataFrame from the cosine similarity matrix for better visualization\n",
    "similarity_df = pd.DataFrame(cosine_similarities, index=word_list, columns=word_list)\n",
    "\n",
    "# Visualize the similarity matrix\n",
    "visualize_similarity_matrix(similarity_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate normalized mean values of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of the absolute values of the embeddings along axis 1\n",
    "mean_embeddings = np.mean(np.abs(word_embeddings_transformer), axis=1)\n",
    "print(\"Normalized Mean values of embeddings:\", mean_embeddings)\n",
    "\n",
    "# Calculate the standard deviation of the embeddings along axis 1\n",
    "std_embeddings = np.std(word_embeddings_transformer, axis=1)\n",
    "print(\"Standard Deviation of embeddings:\", std_embeddings)\n",
    "\n",
    "# Calculate the norm of the embeddings along axis 1\n",
    "norm_embeddings = np.linalg.norm(word_embeddings_transformer, axis=1)\n",
    "print(\"Norm of embeddings:\", norm_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate random vectors with the same mean and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random vectors with the same mean and standard deviation as the word embeddings\n",
    "random_vectors = np.random.normal(loc=np.mean(word_embeddings_transformer),\n",
    "                                  scale=np.std(word_embeddings_transformer),\n",
    "                                  size=word_embeddings_transformer.shape)\n",
    "\n",
    "# Calculate and print the normalized mean values of the random vectors\n",
    "mean_random_vectors = np.mean(np.abs(random_vectors), axis=1)\n",
    "print(\"Normalized Mean values of random vectors:\", mean_random_vectors)\n",
    "\n",
    "# Calculate and print the standard deviation of the random vectors\n",
    "std_random_vectors = np.std(random_vectors, axis=1)\n",
    "print(\"Standard Deviation of random vectors:\", std_random_vectors)\n",
    "\n",
    "# Calculate and print the norm of the random vectors\n",
    "norm_random_vectors = np.linalg.norm(random_vectors, axis=1)\n",
    "print(\"Norm of random vectors:\", norm_random_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the cosine similarity matrix for the random vectors\n",
    "print(\"Cosine Similarity Matrix random vectors:\")\n",
    "cosine_similarities = cosine_similarity(random_vectors)\n",
    "\n",
    "# Display the cosine similarity matrix\n",
    "print(cosine_similarities)\n",
    "\n",
    "# Create a DataFrame for the cosine similarity matrix with appropriate labels\n",
    "similarity_df = pd.DataFrame(\n",
    "    cosine_similarities, \n",
    "    index=[\"Random Vector 1\", \"Random Vector 2\", \"Random Vector 3\", \"Random Vector 4\"], \n",
    "    columns=[\"Random Vector 1\", \"Random Vector 2\", \"Random Vector 3\", \"Random Vector 4\"]\n",
    ")\n",
    "\n",
    "# Visualize the similarity matrix\n",
    "visualize_similarity_matrix(similarity_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## car ~ vehicle + motorcycle - bike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of words to analyze\n",
    "sentences = [\"car\", \"vehicle\", \"motorcycle\", \"bike\"]\n",
    "\n",
    "# Encode the words into embeddings using the embeddings model\n",
    "embeddings = embeddings_model.encode(sentences)\n",
    "\n",
    "# Calculate the cosine similarity between the embedding of \"car\" and the vector operation (vehicle + motorcycle - bike)\n",
    "similarity_score = cosine_similarity(\n",
    "    embeddings[0].reshape(1, -1), \n",
    "    (embeddings[1] + embeddings[2] - embeddings[3]).reshape(1, -1)\n",
    ")[0, 0]\n",
    "\n",
    "# Print the similarity score\n",
    "print(similarity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greece ~ Athens + Italy - Rome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of words for the analogy task\n",
    "sentences = [\"Greece\", \"Athens\", \"Italy\", \"Rome\"]\n",
    "\n",
    "# Encode the words into embeddings\n",
    "embeddings = embeddings_model.encode(sentences)\n",
    "\n",
    "# Calculate and print the cosine similarity for the analogy: Greece ~ Athens + Italy - Rome\n",
    "similarity_score = cosine_similarity(\n",
    "    embeddings[0].reshape(1, -1), \n",
    "    (embeddings[1] + embeddings[2] - embeddings[3]).reshape(1, -1)\n",
    ")[0, 0]\n",
    "print(similarity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So embeddings work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sentences = [\n",
    "    # Interrelated sentences - group 1\n",
    "    \"The data is preprocessed to remove noise and outliers.\",\n",
    "    \"Noise and outliers are eliminated during data preprocessing.\",\n",
    "    \"Preprocessing cleans the data by filtering out noise and irregularities.\",\n",
    "\n",
    "    # Interrelated sentences - group 2\n",
    "    \"Paris is the capital of France.\",\n",
    "    \"Athens is the capital of Greece.\",\n",
    "    \"Rome is the capital of Italy.\"\n",
    "]\n",
    "my_embeddings = embeddings_model.encode(my_sentences)\n",
    "similarity_matrix = cosine_similarity(my_embeddings)\n",
    "print(similarity_matrix)\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=my_sentences, columns=my_sentences)\n",
    "visualize_similarity_matrix(similarity_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "my_model = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(my_model,\n",
    "                                          token=os.environ[\"HF_TOKEN\"],\n",
    "                                          cache_dir=os.environ[\"HF_HOME\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/learn/llm-course/en/chapter6/5\n",
    "\n",
    "In LLaMA and similar Byte-Pair Encoding (BPE) based models:\n",
    "\n",
    "> **tokens ≠ words** (exactly),\n",
    "\n",
    "> **tokens ≈ pieces of words + punctuation + space markers**\n",
    "\n",
    "This helps the model handle any language efficiently with a smaller vocabulary.\n",
    "\n",
    "**Example of subwords**\n",
    "\n",
    "Take the word:\n",
    "`unbelievable`\n",
    "\n",
    "A tokenizer might split it like this:\n",
    "\n",
    "```\n",
    "['un', 'believ', 'able']\n",
    "```\n",
    "\n",
    "* `\"un\"` → a common prefix\n",
    "* `\"believ\"` → root of \"believe\", \"believer\", etc.\n",
    "* `\"able\"` → a common suffix\n",
    "\n",
    "This way, even if `\"unbelievable\"` was never seen during training, the model knows the meaning from its parts.\n",
    "\n",
    "---\n",
    "In the follwoing example:\n",
    "\n",
    "words: \"Hello\", \"world\", \"Let\", \"tokenize\", \"this\", \"text\"\n",
    "\n",
    "punctuation: \",\", \"!\", \".\", \"'s\"\n",
    "\n",
    "space indicators: the Ġ marks the start of a new word with a space. The Ġ symbol is not a space itself, but it indicates that a space precedes the token. This is a convention used in the LLaMA tokenizer (and some others like RoBERTa).\n",
    "\n",
    "Hence `','` and `'Ġ,'` **are different tokens** in LLaMA-style or BPE-style tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(\"Hello, world! Let's tokenize this text.\")\n",
    "print(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
