{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_corpus = [\n",
    "    \"One-Hot Encoding Binary vectors\",\n",
    "    \n",
    "    \"Bag of Words Counts words\",\n",
    "    \n",
    "    \"TF-IDF Weights words by importance\",\n",
    "    \n",
    "    \"N-grams Captures words sequences\",\n",
    "    \n",
    "    \"Words Embeddings Dense vectors\"\n",
    "]\n",
    "my_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the corpus\n",
    "tokenized_corpus = [doc.lower().split() for doc in my_corpus]\n",
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique words\n",
    "all_words = sorted(list(set(word for doc in tokenized_corpus for word in doc)))\n",
    "print(f\"Vocabulary size: {len(all_words)}\")\n",
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "one_hot_word_vectors = np.eye(len(all_words))\n",
    "one_hot_word_vectors_df = pd.DataFrame(one_hot_word_vectors, columns=all_words)\n",
    "one_hot_word_vectors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_vectors = np.zeros((len(tokenized_corpus), len(all_words)))\n",
    "\n",
    "for i, doc in enumerate(tokenized_corpus):\n",
    "    for word in doc:\n",
    "        for j, w in enumerate(all_words):\n",
    "            if w == word:\n",
    "                corpus_vectors[i, j] = 1\n",
    "\n",
    "corpus_vectors_df = pd.DataFrame(corpus_vectors, columns=all_words)\n",
    "corpus_vectors_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Bag of Words representation \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the corpus\n",
    "bow_matrix = count_vectorizer.fit_transform(my_corpus)\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "bow_df = pd.DataFrame(\n",
    "    bow_matrix.toarray(),\n",
    "    columns=count_vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "# Display the Bag of Words representation\n",
    "print(\"Bag of Words representation:\")\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the corpus\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(my_corpus)\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(),\n",
    "    columns=tfidf_vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "# Display the TF-IDF representation\n",
    "print(\"TF-IDF representation:\")\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the CountVectorizer with n-gram range\n",
    "# ngram_range : is a tuple of two integers (min_n, max_n)\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "# Fit and transform the corpus\n",
    "ngram_matrix = ngram_vectorizer.fit_transform(my_corpus)\n",
    "ngram_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for better visualization\n",
    "ngram_df = pd.DataFrame(\n",
    "    ngram_matrix.toarray(),\n",
    "    columns=ngram_vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "# Display the N-grams representation\n",
    "print(\"N-grams shape:\", ngram_matrix.shape)\n",
    "ngram_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Word Embeddings (Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Initialize the Word2Vec model\n",
    "word2vec_model = Word2Vec(tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Get the word embeddings\n",
    "word_embeddings = word2vec_model.wv\n",
    "\n",
    "# Print the word vectors\n",
    "print(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the dimensionality of the vectors\n",
    "print(\"\\nVector dimensionality:\", word_embeddings.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the vector for a specific word\n",
    "print(\"\\nVector for 'embeddings':\")\n",
    "print(word_embeddings['embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also find similar words\n",
    "print(\"\\nWords similar to 'embeddings':\")\n",
    "similar_words = word_embeddings.most_similar('embeddings', topn=3)\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_embeddings.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotly heatmap\n",
    "import plotly.express as px\n",
    "def visualize_similarity_matrix(similarity_df):\n",
    "    fig = px.imshow(similarity_df, labels=dict(x=\"Words\", y=\"Words\", color=\"Similarity\"), x=similarity_df.columns, y=similarity_df.index, color_continuous_scale=\"Viridis\")\n",
    "    fig.update_layout(title=\"Word Similarity Matrix\", xaxis_tickangle=-45, width=800, height=800)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity matrix\n",
    "# Create a similarity matrix manually since KeyedVectors doesn't have similarity_matrix attribute\n",
    "import numpy as np\n",
    "words = word_embeddings.index_to_key\n",
    "similarity_matrix = np.zeros((len(words), len(words)))\n",
    "\n",
    "for i, word1 in enumerate(words):\n",
    "    for j, word2 in enumerate(words):\n",
    "        if word1 != word2:\n",
    "            similarity_matrix[i, j] = word_embeddings.similarity(word1, word2)\n",
    "\n",
    "# Create a DataFrame for the similarity matrix\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=words, columns=words)\n",
    "visualize_similarity_matrix(similarity_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_2d_plot(df):\n",
    "    # Create a scatter plot using Plotly\n",
    "    fig = px.scatter(df, x='C1', y='C2', text='doc', title='Visualization of Word Embeddings', labels=[\"Component 1\", \"Component 2\"])\n",
    "\n",
    "    # Improve the layout\n",
    "    fig.update_traces(textposition='top center', marker=dict(size=10, opacity=0.8), mode='markers+text')\n",
    "    fig.update_layout(width=900, height=700, xaxis=dict(title='Component 1'), yaxis=dict(title='Component 2'))\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA plot\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Initialize PCA with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit PCA on the word embeddings\n",
    "pca.fit(word_embeddings.vectors)\n",
    "\n",
    "# Transform the word embeddings using PCA\n",
    "word_embeddings_2d = pca.transform(word_embeddings.vectors)\n",
    "\n",
    "# Create a DataFrame for the 2D embeddings\n",
    "pca_df = pd.DataFrame(\n",
    "    word_embeddings_2d,\n",
    "    columns=['C1', 'C2']\n",
    ")\n",
    "pca_df['doc'] = words\n",
    "visualize_2d_plot(pca_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSNE plot\n",
    "# t-SNE tries to preserve local relationships, not the global structure. For a small number of points (e.g., ~20 words), t-SNE often:\n",
    "# Overemphasizes tiny distances\n",
    "# Blows up or distorts distances between points not in a neighborhood\n",
    "# Gives unpredictable layouts that \"feel random\"\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Initialize TSNE with 2 components\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "\n",
    "# Fit and transform the word embeddings\n",
    "# Set perplexity to a value less than the number of samples\n",
    "# The perplexity is the number of samples in a neighborhood of a selected point\n",
    "# Default perplexity is 30, so we need to reduce it if we have fewer than 30 samples\n",
    "n_samples = word_embeddings.vectors.shape[0]\n",
    "perplexity = min(30, n_samples - 1)  # Ensure perplexity is less than n_samples\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity, init='pca')\n",
    "word_embeddings_2d = tsne.fit_transform(word_embeddings.vectors)\n",
    "\n",
    "# Create a DataFrame for the 2D embeddings\n",
    "tsne_df = pd.DataFrame(\n",
    "    word_embeddings_2d,\n",
    "    columns=['C1', 'C2']\n",
    ")\n",
    "tsne_df['doc'] = words\n",
    "\n",
    "visualize_2d_plot(tsne_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the embeddings of the sentences in the corpus from the word embeddings\n",
    "# Initialize an empty list to store sentence embeddings\n",
    "sentence_embeddings = np.zeros((len(tokenized_corpus), word_embeddings.vector_size))\n",
    "# Iterate through each document in the corpus\n",
    "for i, doc in enumerate(tokenized_corpus):\n",
    "    # Initialize a numpy array of zeros for the sentence vector\n",
    "    sentence_vector = np.zeros(word_embeddings.vector_size)\n",
    "    word_count = 0\n",
    "    \n",
    "    # Iterate through each word and add its vector to the sentence vector\n",
    "    for word in doc:\n",
    "        if word in word_embeddings:\n",
    "            sentence_vector += word_embeddings[word]\n",
    "            word_count += 1\n",
    "    \n",
    "    # If we found words in the model, calculate the average\n",
    "    if word_count > 0:\n",
    "        sentence_vector = sentence_vector / word_count\n",
    "    \n",
    "    # Add the sentence embedding to our list\n",
    "    sentence_embeddings[i] = sentence_vector\n",
    "# Create a DataFrame with the sentence embeddings\n",
    "# The error occurs because word_embeddings.index_to_key has 17 items but our vectors have 100 dimensions\n",
    "# We need to create column names that match the dimensions of our vectors\n",
    "sentence_embeddings_df = pd.DataFrame(\n",
    "    sentence_embeddings,\n",
    "    columns=[f\"dim_{i}\" for i in range(word_embeddings.vector_size)]\n",
    ")\n",
    "sentence_embeddings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity matrix for document embeddings\n",
    "# Create a similarity matrix manually since KeyedVectors doesn't have similarity_matrix attribute\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity_matrix = np.zeros((len(my_corpus), len(my_corpus)))\n",
    "for i, embedding_i in enumerate(sentence_embeddings):\n",
    "    for j, embedding_j in enumerate(sentence_embeddings):\n",
    "        if i != j:\n",
    "            similarity_matrix[i, j] = cosine_similarity(embedding_i.reshape(1, -1), embedding_j.reshape(1, -1))[0, 0]\n",
    "\n",
    "# Create a DataFrame for the similarity matrix\n",
    "doc_names = [\"doc_\" + str(i+1) for i in range(len(tokenized_corpus))]\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=doc_names, columns=doc_names)\n",
    "visualize_similarity_matrix(similarity_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PCA for document embeddings\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit PCA on the word embeddings\n",
    "pca.fit(sentence_embeddings)\n",
    "\n",
    "# Transform the word embeddings using PCA\n",
    "sentence_embeddings_2d = pca.transform(sentence_embeddings)\n",
    "\n",
    "# Create a DataFrame for the 2D embeddings\n",
    "pca_df = pd.DataFrame(\n",
    "    sentence_embeddings_2d,\n",
    "    columns=['C1', 'C2']\n",
    ")\n",
    "pca_df['doc'] = [\"doc_\" + str(i) for i in range(len(tokenized_corpus))]\n",
    "visualize_2d_plot(pca_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import torch\n",
    "import os\n",
    "if torch.cuda.device_count()>0:\n",
    "    my_device = \"cuda\"\n",
    "    print(f\"You have {torch.cuda.device_count()} GPUs available.\")\n",
    "else:\n",
    "    my_device = \"cpu\"\n",
    "    print(\"You have no GPUs available. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2',\n",
    "                                       token=os.environ[\"HF_TOKEN\"],\n",
    "                                       cache_folder=os.environ[\"HF_HOME\"],\n",
    "                                       device=my_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings_transformer = embeddings_model.encode(my_corpus)\n",
    "print(word_embeddings_transformer.shape)\n",
    "word_embeddings_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = np.zeros((len(my_corpus), len(my_corpus)))\n",
    "for i, embedding_i in enumerate(word_embeddings_transformer):\n",
    "    for j, embedding_j in enumerate(word_embeddings_transformer):\n",
    "        if i != j:\n",
    "            similarity_matrix[i, j] = cosine_similarity(embedding_i.reshape(1, -1), embedding_j.reshape(1, -1))[0, 0]\n",
    "\n",
    "# Create a DataFrame for the similarity matrix\n",
    "doc_names = [\"doc_\" + str(i+1) for i in range(len(tokenized_corpus))]\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=doc_names, columns=doc_names)\n",
    "visualize_similarity_matrix(similarity_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
