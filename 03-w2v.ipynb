{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing: Create a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_corpus = [\"python code\", \"c code\", \"hi there\", \"hi all\"]\n",
    "tokens = set(\" \".join(my_corpus).split())  # Create a set of unique words from the corpus\n",
    "tokens = sorted(tokens)  # Sort the tokens alphabetically\n",
    "tokens  # Display the sorted list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping each word to a unique index\n",
    "word_to_idx = {word: idx for idx, word in enumerate(tokens)}\n",
    "word_to_idx  # Display the word-to-index mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokens)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2  # Size of the hidden layer\n",
    "learning_rate = 0.01 # learning rate for the training\n",
    "nof_epochs = 10000 # number of epochs for the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.rand(vocab_size, N) # input->hidden layer weights\n",
    "W_prime = np.random.rand(N, vocab_size) # hidden->output layer weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(word_idx, vocab_size):\n",
    "    # Initialize a zero vector with the size of the vocabulary\n",
    "    one_hot_vector = np.zeros(vocab_size)\n",
    "    # Set the index corresponding to the word to 1\n",
    "    one_hot_vector[word_idx] = 1\n",
    "    # Return the one-hot encoded vector\n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))  # Stability improvement for softmax\n",
    "    return e_x / e_x.sum(axis=0) # normalize the vector\n",
    "    # return np.exp(x)/np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_all = []  # List to store loss values for each epoch\n",
    "for epoch in range(nof_epochs):  # Iterate over the number of epochs\n",
    "    loss = 0  # Initialize loss for the current epoch\n",
    "    for sentence in my_corpus:  # Iterate over each sentence in the corpus\n",
    "        words = sentence.split()  # Split the sentence into words\n",
    "        for i, target_word in enumerate(words):  # Iterate over each word in the sentence\n",
    "            # Determine the context word based on the position of the target word\n",
    "            if i == 1:\n",
    "                context_word = words[0]  # Context is the previous word\n",
    "            elif i == 0:\n",
    "                context_word = words[1]  # Context is the next word\n",
    "            else:\n",
    "                context_word = None  # No context for other positions\n",
    "\n",
    "            if context_word:  # Proceed if a context word is defined\n",
    "                # One-hot encode the context word\n",
    "                context_idx = word_to_idx[context_word]  # Get index of context word\n",
    "                target_idx = word_to_idx[target_word]  # Get index of target word\n",
    "                x = one_hot_encode(context_idx, vocab_size)  # One-hot encode context\n",
    "\n",
    "                # Forward pass through the network\n",
    "                h = W.T @ x  # Compute hidden layer activations\n",
    "                u = W_prime.T @ h  # Compute output layer activations\n",
    "                y_pred = softmax(u)  # Apply softmax to get predictions\n",
    "\n",
    "                # Backpropagation to compute gradients\n",
    "                e = y_pred.copy()  # Copy predictions to compute error\n",
    "                e[target_idx] -= 1  # Subtract 1 from the true target index\n",
    "\n",
    "                # Compute loss using negative log likelihood\n",
    "                loss += -np.log(y_pred[target_idx] + 1e-8)  # Accumulate loss\n",
    "\n",
    "                # Compute gradients for weight updates\n",
    "                dW_prime = np.outer(h, e)  # Gradient for W_prime\n",
    "                dW = np.outer(x, W_prime @ e)  # Gradient for W\n",
    "\n",
    "                # Update weights using the computed gradients\n",
    "                W_prime -= learning_rate * dW_prime  # Update W_prime\n",
    "                W -= learning_rate * dW  # Update W\n",
    "\n",
    "    loss_all.append(loss)  # Append the loss for the current epoch\n",
    "\n",
    "    if epoch % 1000 == 0:  # Print loss every 1000 epochs\n",
    "        print(f'Epoch: {epoch}, Loss: {loss:.4f}')  # Display epoch and loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display learned word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each word and its corresponding index in the word_to_idx dictionary\n",
    "for word, idx in word_to_idx.items():\n",
    "    # Print the word and its associated vector from the weight matrix W\n",
    "    print(f\"Word: {word}, Vector: {W[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(4, 3)) # set the size of the plot\n",
    "plt.plot(loss_all) # plot the loss over the epochs\n",
    "plt.xlabel('Epochs') # label the x-axis\n",
    "plt.ylabel('Loss') # label the y-axis\n",
    "plt.title('Training Loss Over Epochs') # title of the plot\n",
    "plt.show() # show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in word_to_idx:\n",
    "    print(word, word_to_idx[word]) # print the word and its index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract word vectors from the weight matrix W using the word indices\n",
    "word_vectors = np.array([W[word_to_idx[word]] for word in word_to_idx])\n",
    "# Display the word vectors\n",
    "word_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute all-by-all similarities for the learned word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cosine similarities between all pairs of word vectors\n",
    "similarities = cosine_similarity(word_vectors)\n",
    "# Display the similarity matrix\n",
    "similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the matrix\n",
    "# Import necessary libraries for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a figure with specified size\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot a heatmap of the similarity matrix with word labels on both axes\n",
    "sns.heatmap(similarities, xticklabels=word_to_idx.keys(), yticklabels=word_to_idx.keys(), cmap='binary', annot=True)\n",
    "\n",
    "# Set the title of the plot\n",
    "plt.title('Word Vector Similarities')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install adjustText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D map of word_vectors\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text  # You'll need to: pip install adjustText\n",
    "\n",
    "# Create a new figure with a specified size\n",
    "plt.figure(figsize=(5, 4))\n",
    "\n",
    "# Initialize a list to store text annotations\n",
    "texts = []\n",
    "\n",
    "# Iterate over each word and its corresponding 2D coordinates\n",
    "for word, (x, y) in zip(word_to_idx.keys(), word_vectors):\n",
    "    # Plot the word as a point on the scatter plot\n",
    "    plt.scatter(x, y)\n",
    "    # Add the word as a text annotation at its coordinates\n",
    "    texts.append(plt.text(x, y, word, fontsize=12))\n",
    "\n",
    "# Adjust the text annotations to avoid overlap, using arrows for clarity\n",
    "adjust_text(texts, arrowprops=dict(arrowstyle='->', color='red', lw=0.5))\n",
    "\n",
    "# Set the title of the plot\n",
    "plt.title('2D Map of Word Vectors')\n",
    "\n",
    "# Label the x-axis\n",
    "plt.xlabel('Dimension 1')\n",
    "\n",
    "# Label the y-axis\n",
    "plt.ylabel('Dimension 2')\n",
    "\n",
    "# Enable the grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
