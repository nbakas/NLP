{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing: Create a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_corpus = [\"python code\", \"c code\", \"hi there\", \"hi all\"]\n",
    "tokens = set(\" \".join(my_corpus).split())  # Create a set of unique words\n",
    "tokens = sorted(tokens)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = {word: idx for idx, word in enumerate(tokens)}\n",
    "word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokens)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2  # Size of the hidden layer\n",
    "learning_rate = 0.01\n",
    "nof_epochs = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.rand(vocab_size, N) # input->hidden layer weights\n",
    "W_prime = np.random.rand(N, vocab_size) # hidden->output layer weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(word_idx, vocab_size):\n",
    "    one_hot_vector = np.zeros(vocab_size)\n",
    "    one_hot_vector[word_idx] = 1\n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))  # Stability improvement for softmax\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "    # return np.exp(x)/np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_all = []\n",
    "for epoch in range(nof_epochs):  # Run for a fixed number of epochs\n",
    "    loss = 0\n",
    "    for sentence in my_corpus:\n",
    "        words = sentence.split()\n",
    "        for i, target_word in enumerate(words):\n",
    "            # Define context and target\n",
    "            # context_word = words[i-1] if i-1 >= 0 else None\n",
    "            if i==1:\n",
    "                context_word = words[0]\n",
    "            elif i==0:\n",
    "                context_word = words[1]\n",
    "            else:\n",
    "                context_word = None\n",
    "\n",
    "\n",
    "            if context_word:\n",
    "                # One-hot encode\n",
    "                context_idx = word_to_idx[context_word]\n",
    "                target_idx = word_to_idx[target_word]\n",
    "                x = one_hot_encode(context_idx, vocab_size)\n",
    "\n",
    "                # Forward pass\n",
    "                h = W.T @ x\n",
    "                u = W_prime.T @ h\n",
    "                y_pred = softmax(u)\n",
    "\n",
    "\n",
    "                # Backpropagation\n",
    "                e = y_pred.copy()\n",
    "                e[target_idx] -= 1  # y_pred - y_true, y_true=1\n",
    "\n",
    "                # Loss (negative log likelihood)\n",
    "                loss += -np.log(y_pred[target_idx] + 1e-8)\n",
    "\n",
    "\n",
    "                # print(i, context_word, target_word, loss)\n",
    "\n",
    "                # Gradient for W_prime and W\n",
    "                dW_prime = np.outer(h, e)\n",
    "                dW = np.outer(x, W_prime @ e)\n",
    "\n",
    "                # Update weights\n",
    "                W_prime -= learning_rate * dW_prime\n",
    "                W -= learning_rate * dW\n",
    "\n",
    "    loss_all.append(loss)\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Epoch: {epoch}, Loss: {loss:.4f}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display learned word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, idx in word_to_idx.items():\n",
    "    print(f\"Word: {word}, Vector: {W[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(loss_all)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in word_to_idx:\n",
    "    print(word, word_to_idx[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = np.array([W[word_to_idx[word]] for word in word_to_idx])\n",
    "word_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute all-by-all similarities for the learned word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = cosine_similarity(word_vectors)\n",
    "similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(similarities, xticklabels=word_to_idx.keys(), yticklabels=word_to_idx.keys(), cmap='binary', annot=True)\n",
    "plt.title('Word Vector Similarities')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install adjustText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D map of word_vectors\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text  # You'll need to: pip install adjustText\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "texts = []\n",
    "for word, (x, y) in zip(word_to_idx.keys(), word_vectors):\n",
    "    plt.scatter(x, y)\n",
    "    texts.append(plt.text(x, y, word, fontsize=12))\n",
    "\n",
    "\n",
    "adjust_text(texts, arrowprops=dict(arrowstyle='->', color='red', lw=0.5))\n",
    "\n",
    "plt.title('2D Map of Word Vectors')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
