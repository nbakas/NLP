{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is downloading the notebook from GitHub and running it\n",
    "import requests\n",
    "from pathlib import Path\n",
    "url = \"https://raw.githubusercontent.com/nbakas/NLP/refs/heads/main/02-Preprocessing.ipynb\"\n",
    "filename = url.split(\"/\")[-1]\n",
    "local_path = Path.cwd() / filename\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "local_path.write_bytes(response.content)\n",
    "%run $local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the summaries to a list of sentences\n",
    "my_corpus = df_summary.tolist()\n",
    "print(len(my_corpus))\n",
    "my_corpus[:5]  # Display first 5 sentences to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the corpus\n",
    "tokenized_corpus = [sentence.lower().split() for sentence in my_corpus]\n",
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Create a list of unique words\n",
    "unique_words = [word for sublist in tokenized_corpus for word in sublist]\n",
    "print(len(set(unique_words)))\n",
    "unique_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = Counter(unique_words)\n",
    "word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = [word for word, count in word_counts.most_common(len(set(unique_words)))]\n",
    "unique_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a word index dictionary\n",
    "word_index = {word: idx for idx, word in enumerate(unique_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define context window as a parameter\n",
    "context_window = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the contingency matrix\n",
    "import numpy as np\n",
    "contingency_matrix = np.zeros((len(unique_words), len(unique_words)), dtype=int)\n",
    "\n",
    "# Populate the contingency matrix with the defined context window\n",
    "for iter_sentence, sentence in enumerate(tokenized_corpus):\n",
    "    for i in range(len(sentence)):\n",
    "        for j in range(max(0, i - context_window), min(len(sentence), i + context_window + 1)):\n",
    "            if i != j:\n",
    "                if sentence[i] in word_index and sentence[j] in word_index:\n",
    "                    contingency_matrix[word_index[sentence[i]], word_index[sentence[j]]] += 1\n",
    "    if iter_sentence % 100_000 == 0:\n",
    "        print(iter_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nof_obj = contingency_matrix.shape[0]\n",
    "np.random.seed(0)\n",
    "embeddings_matrix = 2*np.random.rand(nof_obj, embeddings_dim)-1\n",
    "print(embeddings_matrix.shape)\n",
    "embeddings_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_weights_all = []\n",
    "idxs_all = []\n",
    "nof_nearest_neighbors = 5\n",
    "for i in range(nof_obj):\n",
    "    weights = contingency_matrix[i]\n",
    "    idxs = np.arange(len(weights))[np.arange(len(weights)) != i]\n",
    "    most_similar_idxs = np.argsort(weights[idxs])[::-1][:nof_nearest_neighbors]\n",
    "    idxs_all.append(idxs[most_similar_idxs])\n",
    "    sum_weights = np.sum(weights[idxs][most_similar_idxs])\n",
    "    sum_weights_all.append(sum_weights)\n",
    "    if i%1000 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nof_iterations = contingency_matrix.shape[0]//20\n",
    "nof_iterations = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convergence_history = []\n",
    "for _ in range(nof_iterations):\n",
    "    for i in range(nof_obj):\n",
    "        if sum_weights_all[i] != 0:\n",
    "            # prev_opti_xy = deepcopy(embeddings_matrix)\n",
    "            embeddings_matrix[i] = np.sum(contingency_matrix[i][idxs_all[i], None] * embeddings_matrix[idxs_all[i]], axis=0) / sum_weights_all[i]\n",
    "            # convergence_history.append(np.linalg.norm(embeddings_matrix - prev_opti_xy))\n",
    "        if i%1000 == 0:\n",
    "            print(_, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "nof_keep_words = 100\n",
    "# Create a DataFrame for plotting\n",
    "df = pd.DataFrame(embeddings_matrix[:nof_keep_words], columns=['x', 'y'])\n",
    "df['word'] = unique_words[:nof_keep_words]\n",
    "df['count'] = [word_counts[word] for word in unique_words[:nof_keep_words]]  # Assuming word_counts is a dictionary\n",
    "\n",
    "# Create a scatter plot using Plotly with size proportional to word counts\n",
    "fig = px.scatter(df, x='x', y='y', text='word', size='count', \n",
    "                 title='Top 20 Unique Words in Embeddings Space',\n",
    "                 hover_data=['count'])\n",
    "fig.update_traces(textfont_size=15)  # Adjust the font size here\n",
    "\n",
    "# Add pairwise connecting lines with thickness proportional to contingency_matrix\n",
    "max_weight = np.max(contingency_matrix)/10\n",
    "threshold_weight = np.quantile(contingency_matrix[contingency_matrix>0], 0.9999)\n",
    "for i in range(nof_keep_words):\n",
    "    for j in range(i + 1, nof_keep_words):\n",
    "        weight = contingency_matrix[i, j]\n",
    "        if weight > threshold_weight:  \n",
    "            fig.add_trace(go.Scatter(x=[df['x'][i], df['x'][j]], \n",
    "                                     y=[df['y'][i], df['y'][j]], \n",
    "                                     mode='lines',\n",
    "                                     line=dict(width=weight/max_weight, color='rgba(0,0,0,0.2)'),\n",
    "                                     showlegend=False))\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on contingency_matrix find the top 5 words related to 'organic'\n",
    "word_based = 'chocolate'\n",
    "top_related_words = np.argsort(contingency_matrix[word_index[word_based]])[::-1][:5]\n",
    "top_related_words = [unique_words[i] for i in top_related_words]\n",
    "print(top_related_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_summary is a DataFrame with a column 'Summary' containing the text data\n",
    "word1 = 'chocolate'\n",
    "word2 = 'best'\n",
    "word3 = 'dark'\n",
    "\n",
    "# Find all entries in df_summary that include word1, word2, or word3\n",
    "matching_entries = df_summary[df_summary.apply(lambda x: all(word in x for word in [word1, word2, word3]))]\n",
    "\n",
    "# Display the matching entries\n",
    "print(matching_entries)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
