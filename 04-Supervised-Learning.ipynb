{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This will overwrite 02-Preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is downloading the notebook from GitHub and running it\n",
    "import requests\n",
    "from pathlib import Path\n",
    "url = \"https://raw.githubusercontent.com/nbakas/NLP/refs/heads/main/02-Preprocessing.ipynb\"\n",
    "filename = url.split(\"/\")[-1]\n",
    "local_path = Path.cwd() / filename\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "local_path.write_bytes(response.content)\n",
    "%run $local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_summary, df_score, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=384,  # Limit features to reduce dimensionality\n",
    "    min_df=5,           # Minimum document frequency\n",
    "    max_df=0.8,         # Maximum document frequency (ignore terms that appear in >80% of documents)\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train = tfidf_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:5].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.mean(X_train.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Get the number of rows in X_train\n",
    "num_rows = X_train.shape[0]\n",
    "\n",
    "# Generate 10 random indices\n",
    "random_indices = random.sample(range(num_rows), 10)\n",
    "\n",
    "# Print 10 random lines of X_train\n",
    "for idx in random_indices:\n",
    "    print(f\"Index: {idx}, Mean: {np.mean(X_train[idx].toarray()[0], axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the test data\n",
    "X_test = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the shape of the TF-IDF matrices\n",
    "print(f\"Training TF-IDF matrix shape: {X_train.shape}\")\n",
    "print(f\"Testing TF-IDF matrix shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names (words)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print(f\"Number of features (words): {len(feature_names)}\")\n",
    "print(f\"Features: {feature_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequent words\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Get the sum of TF-IDF values for each term across all documents\n",
    "tfidf_means = np.array(X_train.mean(axis=0)).flatten()\n",
    "\n",
    "# Create a DataFrame with terms and their TF-IDF sums\n",
    "term_importance = pd.DataFrame({\n",
    "    'term': feature_names,\n",
    "    'tfidf_mean': tfidf_means\n",
    "})\n",
    "\n",
    "# Sort by importance (TF-IDF sum)\n",
    "term_importance = term_importance.sort_values('tfidf_mean', ascending=False)\n",
    "\n",
    "# Display the top 10 most important terms\n",
    "print(\"Top 10 most important terms:\")\n",
    "print(term_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_summary, df_score, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data for Word2Vec\n",
    "tokenized_train = [text.split() for text in X_train]\n",
    "tokenized_test = [text.split() for text in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# Define the Word2Vec model\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=tokenized_train,\n",
    "    vector_size=384, # Dimensionality of the word vectors\n",
    "    window=5, # Maximum distance between the current and predicted word within a sentence\n",
    "    min_count=2, # Ignores words with frequency lower than this\n",
    "    workers=4, # Number of threads to run in parallel\n",
    "    sg=1, # Training algorithm: 1 for skip-gram; 0 for CBOW\n",
    "    seed=42\n",
    ")\n",
    "print(\"Training Word2Vec model...\")\n",
    "# Train the model\n",
    "w2v_model.train(\n",
    "    tokenized_train, # List of sentences to train\n",
    "    total_examples=len(tokenized_train), # Number of sentences to train on\n",
    "    epochs=10 # Number of epochs \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Vocabulary size: {len(w2v_model.wv.key_to_index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create document vectors by averaging word vectors\n",
    "def document_vector(doc, model):\n",
    "    # Filter words that are in the model vocabulary\n",
    "    doc_words = [word for word in doc if word in model.wv]\n",
    "    if len(doc_words) == 0:\n",
    "        # Return zeros if no words are in vocabulary\n",
    "        return np.zeros(model.vector_size)\n",
    "    # Return the mean of all word vectors in the document\n",
    "    return np.mean([model.wv[word] for word in doc_words], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create document vectors for training and testing sets\n",
    "X_train = np.array([document_vector(doc, w2v_model) for doc in tokenized_train])\n",
    "X_test = np.array([document_vector(doc, w2v_model) for doc in tokenized_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training Word2Vec matrix shape: {X_train.shape}\")\n",
    "print(f\"Testing Word2Vec matrix shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore some word similarities\n",
    "my_test_word = \"delicious\" # Try another common word e.g. food, price, service, etc.\n",
    "try:\n",
    "    # Find words most similar\n",
    "    print(f\"\\nWords most similar to '{my_test_word}':\")\n",
    "    for word, similarity in w2v_model.wv.most_similar(my_test_word, topn=5):\n",
    "        print(f\"{word}: {similarity:.4f}\")\n",
    "except KeyError:\n",
    "    print(f\"Word '{my_test_word}' not in vocabulary. Try another common word.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import torch\n",
    "import os\n",
    "if torch.cuda.device_count()>0:\n",
    "    my_device = \"cuda\"\n",
    "    print(f\"You have {torch.cuda.device_count()} GPUs available.\")\n",
    "else:\n",
    "    my_device = \"cpu\"\n",
    "    print(\"You have no GPUs available. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2',\n",
    "                                       token=os.environ[\"HF_TOKEN\"],\n",
    "                                       cache_folder=os.environ[\"HF_HOME\"],\n",
    "                                       device=my_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################\n",
    "#################################################################################################\n",
    "########## The following cell will take some time (e.g. 20 min on the CPU of a laptop) ##########\n",
    "#################################################################################################\n",
    "#################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings_transformer = embeddings_model.encode(df_summary)\n",
    "print(word_embeddings_transformer.shape)\n",
    "word_embeddings_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(word_embeddings_transformer, df_score, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training Word2Vec matrix shape: {X_train.shape}\")\n",
    "print(f\"Testing Word2Vec matrix shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "# Use 'multinomial' solver for multi-class classification\n",
    "lr_model = LogisticRegression(\n",
    "    multi_class='multinomial',  # Multinomial for multi-class problems\n",
    "    solver='lbfgs',             # Efficient solver for multinomial logistic regression\n",
    "    max_iter=1000,              # Increase max iterations to ensure convergence\n",
    "    random_state=42,            # For reproducibility\n",
    "    n_jobs=-1                   # Use all available cores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Training Logistic Regression model...\")\n",
    "lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nLogistic Regression Model Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "plt.figure(figsize=(5, 4))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=sorted(set(y_test)), \n",
    "            yticklabels=sorted(set(y_test)))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for Logistic Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~45 min!\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "# Evaluate the model\n",
    "print(\"\\nLogistic Regression Model Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "# Display detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "# XGBoost expects classes to start from 0, but our labels are 1-5\n",
    "# Convert labels from 1-5 to 0-4 for training, by subtracting 1\n",
    "xgb_model.fit(X_train, y_train - 1)\n",
    "# Make predictions\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "# Evaluate the model\n",
    "print(\"\\nLogistic Regression Model Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test-1, y_pred):.4f}\")\n",
    "# Display detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test-1, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
