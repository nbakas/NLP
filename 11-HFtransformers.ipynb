{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXZIl4FRstm4"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook demonstrates the use of a language model, specifically the \"meta-llama/Llama-3.2-1B-Instruct\", for generating text and embeddings. It begins by setting up the model and tokenizer using the Hugging Face Transformers library, ensuring that the pad token is correctly defined. The notebook then illustrates how to generate text using the model in both streaming and non-streaming modes. It applies a chat template to user messages, moves inputs to a GPU if available, and generates outputs with a specified maximum number of tokens. The generated text is cleaned to remove system messages, and the time taken for generation is displayed.\n",
        "In addition to text generation, the notebook explores embeddings using the SentenceTransformer library. It encodes words and sentences to compute cosine similarity matrices, which are visualized to show relationships between different words and sentences. The notebook also demonstrates the concept of Retrieval Augmented Generation (RAG) by encoding a user's question and sorting sentences based on their similarity to the question. This approach helps in retrieving relevant information from a text corpus. Finally, the notebook sets up a pipeline for generating responses to user queries, showcasing the integration of text generation and retrieval techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqMNyIA_Orvh"
      },
      "source": [
        "## Environment Variables\n",
        "we will need to use Environment Variables:\n",
        "- HF_TOKEN is you huggingface token, you may generate one on this url: https://huggingface.co/settings/tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcdbhQ1s_U1h"
      },
      "source": [
        "We will use Llama-3.2-1B-Instruct. In order to be able to download it you will need to accept the terms of use on the following link:\n",
        "\n",
        "https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\n",
        "\n",
        "You can check if you have been granted on:\n",
        "\n",
        "https://huggingface.co/settings/gated-repos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XegLKNxhWtI"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBn7UR-2KRKD"
      },
      "source": [
        "https://huggingface.co/docs/transformers/index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "dekk105Xef1m"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, pipeline\n",
        "import os\n",
        "import torch\n",
        "import time\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hXqF4y6fdfz",
        "outputId": "7a712cd5-82b8-49a5-bec2-1471b0262c52"
      },
      "outputs": [],
      "source": [
        "# If HF_TOKEN is not set, prompt the user to enter it securely\n",
        "# import getpass\n",
        "# os.environ['HF_TOKEN'] = getpass.getpass(\"Enter the value for HF_TOKEN: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RzxgspCO2w9"
      },
      "source": [
        "## HF_HOME\n",
        "is the directory where you want to save models' weights. export $HF_HOME as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "OAVuzgKNl15o"
      },
      "outputs": [],
      "source": [
        "# E.g. on colab do:\n",
        "# os.environ[\"HF_HOME\"] = \"/content/my_huggingface_cache\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euluA0sDhs7_"
      },
      "source": [
        "# The Transformers Library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eKFO83tha48"
      },
      "source": [
        "## Download Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "3whaClpJfMWu"
      },
      "outputs": [],
      "source": [
        "my_model = \"meta-llama/Llama-3.2-1B-Instruct\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "CfX_L0I9faPg"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(my_model,\n",
        "                                          token=os.environ[\"HF_TOKEN\"],\n",
        "                                          cache_dir=os.environ[\"HF_HOME\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "oZvIpXkuf-e3"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(   my_model,\n",
        "                                                token=os.environ[\"HF_TOKEN\"],\n",
        "                                                cache_dir=os.environ[\"HF_HOME\"],\n",
        "                                                device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTHqWqL8nIXc"
      },
      "source": [
        "## pad token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wRt6bvvgJsk"
      },
      "outputs": [],
      "source": [
        "# Depending on the model, the pad token might not be defined\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(\"Pad token was None, so it was set to eos token.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvIbkoAPnCWD"
      },
      "source": [
        "## Streamer for model.generate and pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "tEbL3_aYgVcz"
      },
      "outputs": [],
      "source": [
        "streamer = TextStreamer(tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjJZG-hahokL"
      },
      "source": [
        "## Messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "6VuFgHy6gY-D"
      },
      "outputs": [],
      "source": [
        "system_instructions = f\"You are a helpful assistant.\"\n",
        "my_messages = [{\"role\": \"system\", \"content\": system_instructions}]\n",
        "my_prompt = \"\"\"What is an LLM?\"\"\"\n",
        "my_messages.append({\"role\": \"user\", \"content\": my_prompt})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GzZDHvkhzlH"
      },
      "source": [
        "## Streaming Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYnz0L3TnWvX"
      },
      "source": [
        "### Apply chat template to messages and return tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDvLZlCygb_s",
        "outputId": "ca0e7fbe-b923-4346-fb44-d63f9bd5ea99"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer.apply_chat_template(my_messages, return_tensors=\"pt\")\n",
        "print(type(inputs)) # <class 'torch.Tensor'>\n",
        "attention_mask = (inputs != tokenizer.pad_token_id).long()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTswWQMpnbRU"
      },
      "source": [
        "### Move inputs to GPU if available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPlVQFnfnabR",
        "outputId": "b30460ea-e25c-4f47-c524-c4e414831c3a"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.device_count()>0:\n",
        "    inputs = inputs.to(\"cuda\")\n",
        "    attention_mask = attention_mask.to(\"cuda\")\n",
        "    print(\"Inputs and Attention Mask transfered to CUDA\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcaH0TNUgoM1",
        "outputId": "c9e815f3-8d06-4c69-e941-d7e55950ad36"
      },
      "outputs": [],
      "source": [
        "t1 = time.time()\n",
        "MAXIMUM_TOKENS = 128\n",
        "outputs = model.generate(inputs,\n",
        "                         streamer=streamer,\n",
        "                         pad_token_id=tokenizer.eos_token_id,\n",
        "                         attention_mask=attention_mask,\n",
        "                         max_new_tokens=MAXIMUM_TOKENS)\n",
        "t2 = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(type(outputs)) # <class 'torch.Tensor'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bkfh-CY3nvzL"
      },
      "source": [
        "### Clean the sesponse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vruyOo26gspc",
        "outputId": "a36a7e92-c4ef-4c3d-8baf-ff039c2178fe"
      },
      "outputs": [],
      "source": [
        "# To ommit <|begin_of_text|><|start_header_id|>system<|end_header_id|> we use:\n",
        "generated_text = tokenizer.decode(outputs[0],\n",
        "                                  skip_special_tokens=True,\n",
        "                                  clean_up_tokenization_spaces=True)\n",
        "print(f\"{generated_text}\\n\\n{(t2-t1)/60:.2f} minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(type(generated_text)) # <class 'str'>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "301kyfgnhIrO",
        "outputId": "f02fadcf-0274-4e69-9cb3-fea0556f4f7c"
      },
      "outputs": [],
      "source": [
        "# To omit the system message we use:\n",
        "cleaned_text = re.sub(r\"^.*?assistant\\n\\n\", \"\", generated_text, flags=re.DOTALL)\n",
        "print(cleaned_text + \"\\n\\n\" + f\"{(t2-t1)/60:.2f} minutes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6srRs8Gvh4m4"
      },
      "source": [
        "## Inference (without steaming)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZ1eH4PahLal",
        "outputId": "5bf7700b-a756-4875-fc19-ffe32e79d628"
      },
      "outputs": [],
      "source": [
        "t1 = time.time()\n",
        "MAXIMUM_TOKENS = 128\n",
        "outputs = model.generate(inputs,\n",
        "                         pad_token_id=tokenizer.eos_token_id,\n",
        "                         attention_mask=attention_mask,\n",
        "                         max_new_tokens=MAXIMUM_TOKENS)\n",
        "t2 = time.time()\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(generated_text + \"\\n\\n\" + f\"{(t2-t1)/60:.2f} minutes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp3KXwWzh8-z"
      },
      "source": [
        "## The Pipeline object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4S4cv10ehOky",
        "outputId": "28d1ce04-fdda-4f01-b1e5-5ff6bfa452ab"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline(\"text-generation\",\n",
        "                model=model,\n",
        "                tokenizer=tokenizer,\n",
        "                device_map=\"auto\")\n",
        "t1 = time.time()\n",
        "MAXIMUM_TOKENS = 128\n",
        "outputs = pipe(my_messages,\n",
        "               max_new_tokens=MAXIMUM_TOKENS,\n",
        "               pad_token_id=pipe.tokenizer.eos_token_id,\n",
        "               streamer=streamer)\n",
        "t2 = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8weNIdj9DG4B",
        "outputId": "f147fef7-e54e-461f-f0b5-dfc9a7eafb82"
      },
      "outputs": [],
      "source": [
        "# In pipeline outputs (not in model.generate) we have the \"generated_text\" attribute:\n",
        "print(outputs[0][\"generated_text\"][-1]['content'] + \"\\n\\n\" + f\"{(t2-t1)/60:.2f} minutes\")\n",
        "# [{'generated_text': [{'role': 'system', 'content': 'You are a helful assistant.'},\n",
        "#                       {'role': 'user', 'content': \"...\n",
        "type(outputs) # <class 'list'>\n",
        "type(outputs[0]) # <class 'dict'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaULB1B1iAVF"
      },
      "source": [
        "# Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGJsMrqFiH5P"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpcfmiZXiBpf",
        "outputId": "c5a1221f-5dc9-4605-e9f0-4fd2ee46589f"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch\n",
        "import os\n",
        "if torch.cuda.device_count()>0:\n",
        "    my_device = \"cuda\"\n",
        "    print(f\"You have {torch.cuda.device_count()} GPUs available.\")\n",
        "else:\n",
        "    my_device = \"cpu\"\n",
        "    print(\"You have no GPUs available. Running on CPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKLpQL7niN3z"
      },
      "source": [
        "## The SentenceTransformer object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "ttfZb9iYiKcI"
      },
      "outputs": [],
      "source": [
        "embeddings_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2',\n",
        "                                       token=os.environ[\"HF_TOKEN\"],\n",
        "                                       cache_folder=os.environ[\"HF_HOME\"],\n",
        "                                       device=my_device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSDdVdidva_2"
      },
      "source": [
        "## Function to visualizing the similarity matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "_XKOzzvGvWWm"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def visualize_similarity_matrix(similarity_matrix, items_labels, mat_size=5):\n",
        "    for i in range(similarity_matrix.shape[0]):\n",
        "        similarity_matrix[i,i] = 0\n",
        "    plt.figure(figsize=(mat_size, mat_size))\n",
        "    plt.imshow(similarity_matrix, interpolation='nearest', cmap='viridis')\n",
        "    plt.colorbar(label=\"Cosine Similarity\")\n",
        "    plt.xticks(ticks=np.arange(len(items_labels)), labels=items_labels, rotation=90, fontsize=8)\n",
        "    plt.yticks(ticks=np.arange(len(items_labels)), labels=items_labels, fontsize=8)\n",
        "    plt.title(\"Cosine Similarity Matrix\", fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_9_SD2OimHP"
      },
      "source": [
        "## Test Embeddings - unrelated words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-_cQY_sFhcQ",
        "outputId": "3719a5f1-1a71-4760-8c7c-7f0c00355acd"
      },
      "outputs": [],
      "source": [
        "word_list = [\"reciprocal\", \"obfuscate\", \"hyperbolic\", \"tensor\"]\n",
        "word_embeddings = embeddings_model.encode(word_list)\n",
        "word_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "b3luGfI7iSRM",
        "outputId": "7e50b00a-4f58-4fb2-80f4-25c35370cdb9"
      },
      "outputs": [],
      "source": [
        "cosine_similarities = cosine_similarity(word_embeddings)\n",
        "print(\"Cosine Similarity Matrix:\")\n",
        "print(cosine_similarities)\n",
        "visualize_similarity_matrix(cosine_similarities, word_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alZvEv3Biv5L"
      },
      "source": [
        "## Test Embeddings - related words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "6M2B2P9qiqVu",
        "outputId": "aad6f1b1-d7a5-495d-de4e-7ab3ee94aa66"
      },
      "outputs": [],
      "source": [
        "word_list = [\"book\", \"book!\", \"publication\", \"article\"]\n",
        "word_embeddings = embeddings_model.encode(word_list)\n",
        "cosine_similarities = cosine_similarity(word_embeddings)\n",
        "print(\"Cosine Similarity Matrix:\")\n",
        "print(cosine_similarities)\n",
        "visualize_similarity_matrix(cosine_similarities, word_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ethNB13Ri2pv"
      },
      "source": [
        "## Calculate normalized mean values of embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8701DcBaivhF",
        "outputId": "caad690a-b170-442e-d4ca-fbdfb2dd2aee"
      },
      "outputs": [],
      "source": [
        "mean_embeddings = np.mean(np.abs(word_embeddings), axis=1)\n",
        "print(\"Normalized Mean values of embeddings:\", mean_embeddings)\n",
        "std_embeddings = np.std(word_embeddings, axis=1)\n",
        "print(\"Standard Deviation of embeddings:\", std_embeddings)\n",
        "norm_embeddings = np.linalg.norm(word_embeddings, axis=1)\n",
        "print(\"Norm of embeddings:\", norm_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv8MSkyTi6jJ"
      },
      "source": [
        "## Generate random vectors with the same mean and std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKHd_mEGi3m0",
        "outputId": "446eb1ac-987a-4f04-e609-f78580351e9a"
      },
      "outputs": [],
      "source": [
        "random_vectors = np.random.normal(loc=np.mean(word_embeddings),\n",
        "                                  scale=np.std(word_embeddings),\n",
        "                                  size=word_embeddings.shape)\n",
        "mean_random_vectors = np.mean(np.abs(random_vectors), axis=1)\n",
        "print(\"Normalized Mean values of random vectors:\", mean_random_vectors)\n",
        "std_random_vectors = np.std(random_vectors, axis=1)\n",
        "print(\"Standard Deviation of random vectors:\", std_random_vectors)\n",
        "norm_random_vectors = np.linalg.norm(random_vectors, axis=1)\n",
        "print(\"Norm of random vectors:\", norm_random_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "X3RsT8FUTEHO",
        "outputId": "ab61d40d-59b2-4424-c849-5d85dd868a98"
      },
      "outputs": [],
      "source": [
        "print(\"Cosine Similarity Matrix random vectors:\")\n",
        "cosine_similarities = cosine_similarity(random_vectors)\n",
        "print(cosine_similarities)\n",
        "visualize_similarity_matrix(cosine_similarities, [\"Random Vector 1\", \"Random Vector 2\", \"Random Vector 3\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDJOes3HruF9"
      },
      "source": [
        "## car ~ vehicle + motorcycle - bike"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxoOEqnArrvt",
        "outputId": "ed08f1c5-83cb-4b4c-f86c-1d402c2f70db"
      },
      "outputs": [],
      "source": [
        "sentences = [\"car\", \"vehicle\", \"motorcycle\", \"bike\"]\n",
        "embeddings = embeddings_model.encode(sentences)\n",
        "print(cosine_similarity(embeddings[0].reshape(1, -1), (embeddings[1] + embeddings[2] - embeddings[3]).reshape(1, -1))[0, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_ooMezvr1-k"
      },
      "source": [
        "## Greece ~ Athens + Italy - Rome"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaFEGFx0rypQ",
        "outputId": "e3f44292-70d7-47ca-e403-24e8565e6788"
      },
      "outputs": [],
      "source": [
        "sentences = [\"Greece\", \"Athens\", \"Italy\", \"Rome\"]\n",
        "embeddings = embeddings_model.encode(sentences)\n",
        "print(cosine_similarity((embeddings[0]).reshape(1, -1), (embeddings[1]+embeddings[2]-embeddings[3]).reshape(1, -1))[0, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8TzaG8nTH7m"
      },
      "source": [
        "So embeddings work!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufSNtaO5r7UY"
      },
      "source": [
        "## Sentence embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 909
        },
        "id": "6ANZU78xr6Ao",
        "outputId": "e44475c0-2105-4ca8-9f77-8b9f066ca50e"
      },
      "outputs": [],
      "source": [
        "my_sentences = [\n",
        "    # Interrelated sentences - group 1\n",
        "    \"The data is preprocessed to remove noise and outliers.\",\n",
        "    \"Noise and outliers are eliminated during data preprocessing.\",\n",
        "    \"Preprocessing cleans the data by filtering out noise and irregularities.\",\n",
        "\n",
        "    # Interrelated sentences - group 2\n",
        "    \"Paris is the capital of France.\",\n",
        "    \"Athens is the capital of Greece.\",\n",
        "    \"Rome is the capital of Italy.\"\n",
        "]\n",
        "my_embeddings = embeddings_model.encode(my_sentences)\n",
        "similarity_matrix = cosine_similarity(my_embeddings)\n",
        "print(similarity_matrix)\n",
        "visualize_similarity_matrix(similarity_matrix, my_sentences, mat_size=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbb60su0jE7d"
      },
      "source": [
        "# Retrieval Augmented Generation (RAG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "w3kyBQ_Zi7bj"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMl8vUL6jIic",
        "outputId": "01c8fd60-0749-43cf-e602-6b542eb532d0"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.device_count()>0:\n",
        "    my_device = \"cuda\"\n",
        "    print(f\"You have {torch.cuda.device_count()} GPUs available.\")\n",
        "else:\n",
        "    my_device = \"cpu\"\n",
        "    print(\"You have no GPUs available. Running on CPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15wz-R2Jjk0b"
      },
      "source": [
        "## Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "8aI_jbSMjKvN"
      },
      "outputs": [],
      "source": [
        "embeddings_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', token=os.environ[\"HF_TOKEN\"],\n",
        "                                       cache_folder=os.environ[\"HF_HOME\"], device=my_device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzINfM46jozb"
      },
      "source": [
        "## Text for retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "LSXW6j09jOOb"
      },
      "outputs": [],
      "source": [
        "my_text = \"\"\"\n",
        "This notebook demonstrates the use of a language model, specifically the \"meta-llama/Llama-3.2-1B-Instruct\", for generating text and embeddings. It begins by setting up the model and tokenizer using the Hugging Face Transformers library, ensuring that the pad token is correctly defined. The notebook then illustrates how to generate text using the model in both streaming and non-streaming modes. It applies a chat template to user messages, moves inputs to a GPU if available, and generates outputs with a specified maximum number of tokens. The generated text is cleaned to remove system messages, and the time taken for generation is displayed.\n",
        "In addition to text generation, the notebook explores embeddings using the SentenceTransformer library. It encodes words and sentences to compute cosine similarity matrices, which are visualized to show relationships between different words and sentences. The notebook also demonstrates the concept of Retrieval Augmented Generation (RAG) by encoding a user's question and sorting sentences based on their similarity to the question. This approach helps in retrieving relevant information from a text corpus. Finally, the notebook sets up a pipeline for generating responses to user queries, showcasing the integration of text generation and retrieval techniques.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "T8fHlBhkNMhj"
      },
      "outputs": [],
      "source": [
        "my_sentences = my_text.split('.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Rs91HYoRNN5X",
        "outputId": "8c38ad5d-3e92-4d23-821c-a4702806b46e"
      },
      "outputs": [],
      "source": [
        "my_sentences[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxJfHjKqjaDc",
        "outputId": "e57e55a8-8dbb-41fe-a89c-b181084e7d09"
      },
      "outputs": [],
      "source": [
        "my_embeddings = embeddings_model.encode(my_sentences)\n",
        "print(my_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGQYJgWejxfj"
      },
      "source": [
        "## Encode user's question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "XL1kl06Jjedc"
      },
      "outputs": [],
      "source": [
        "my_question = \"What is this notebook about?\"\n",
        "my_question_embedding = embeddings_model.encode([my_question])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQKIbgG3kIg0"
      },
      "source": [
        "## Sort sentences based on the similarity to the question embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhIrKbimkEKM",
        "outputId": "d463bd8b-590d-4417-a827-a5c7f24fb842"
      },
      "outputs": [],
      "source": [
        "similarity_to_question = cosine_similarity(my_question_embedding, my_embeddings).flatten()\n",
        "sorted_indices = similarity_to_question.argsort()[::-1]  # Sort in descending order\n",
        "print(sorted_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w11jHnI0kMaP"
      },
      "source": [
        "## Get sorted sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxOoVK4lkJ_9",
        "outputId": "01ccf7da-43b7-4e9b-e384-581a3b5b47c4"
      },
      "outputs": [],
      "source": [
        "sorted_sentences = [my_sentences[i] for i in sorted_indices]\n",
        "print(\"Sorted sentences based on cosine similarity to the question:\")\n",
        "for i, sentence in enumerate(sorted_sentences):\n",
        "    print(\"-\"*100)\n",
        "    print(f\"Sentence {i+1}, similarity: {similarity_to_question[sorted_indices[i]]:.2f}\")\n",
        "    print(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfwdzDXOkeJb"
      },
      "source": [
        "## Setup messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "5DOEyRaukNVj",
        "outputId": "87558d19-e819-423e-991a-3b43a36e2c1f"
      },
      "outputs": [],
      "source": [
        "nof_keep_sentences = 3\n",
        "system_instructions = f\"You are a helful assistant.\"\n",
        "my_messages = [{\"role\": \"system\", \"content\": system_instructions}]\n",
        "my_prompt = f\"Use the following sentences:\"\n",
        "for sentence in sorted_sentences[:nof_keep_sentences]:\n",
        "    my_prompt += f\"\\n{sentence}\"\n",
        "my_prompt += f\"\\n\\nAnswer the question:\\n\\n'{my_question}'\"\n",
        "my_prompt += \"\\n\\nStart your reply with the word Hello!\"\n",
        "my_messages.append({\"role\": \"user\", \"content\": my_prompt})\n",
        "my_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFaZA4MmkkqZ"
      },
      "source": [
        "## Answer the question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "2obTGEmqkfEp"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, TextStreamer\n",
        "import os\n",
        "my_model = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(my_model,\n",
        "                                          token=os.environ[\"HF_TOKEN\"],\n",
        "                                          cache_dir=os.environ[\"HF_HOME\"])\n",
        "model = AutoModelForCausalLM.from_pretrained(   my_model,\n",
        "                                                token=os.environ[\"HF_TOKEN\"],\n",
        "                                                cache_dir=os.environ[\"HF_HOME\"],\n",
        "                                                device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEilz0-Xkzut",
        "outputId": "f1d33ce4-a178-4457-e391-191c7cff60e1"
      },
      "outputs": [],
      "source": [
        "# Depending on the model, the pad token might not be defined\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(\"Pad token was None, so it was set to eos token.\")\n",
        "\n",
        "streamer = TextStreamer(tokenizer)\n",
        "\n",
        "pipe = pipeline(\"text-generation\",\n",
        "                model=model,\n",
        "                tokenizer=tokenizer,\n",
        "                device_map=\"auto\")\n",
        "MAXIMUM_TOKENS = 128\n",
        "outputs = pipe(my_messages,\n",
        "               max_new_tokens=MAXIMUM_TOKENS,\n",
        "               pad_token_id=pipe.tokenizer.eos_token_id,\n",
        "               streamer=streamer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6jcPV3-o8pz",
        "outputId": "b7f011b4-bc45-45e7-a216-c3db19c7e899"
      },
      "outputs": [],
      "source": [
        "print(outputs[0][\"generated_text\"][-1]['content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMw1TK20qrXS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
