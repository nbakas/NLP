{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXZIl4FRstm4"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuIyEP5RUOYO"
      },
      "source": [
        "This notebook demonstrates the use of large language models for generating text,  embeddings and Retrieval Augmented Generation (RAG). It begins by setting up the model and tokenizer using the Hugging Face Transformers library, ensuring that the pad token is correctly defined. The notebook then illustrates how to generate text using the model in both streaming and non-streaming modes. It applies a chat template to user messages, moves inputs to a GPU if available, and generates outputs with a specified maximum number of tokens. The generated text is cleaned to remove system messages, and the time taken for generation is displayed.\n",
        "In addition to text generation, the notebook explores embeddings using the SentenceTransformer library. It encodes words and sentences to compute cosine similarity matrices, which are visualized to show relationships between different words and sentences. The notebook also demonstrates the concept of RAG by encoding a user's question and sorting sentences based on their similarity to the question. This approach helps in retrieving relevant information from a text corpus. Finally, the notebook sets up a pipeline for generating responses to user queries, showcasing the integration of text generation and retrieval techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnRDOzahswPv"
      },
      "source": [
        "---\n",
        "Throughout the code we will be having hints how to run it on HPC, starting with the **[HPC]** flag.\n",
        "\n",
        "---\n",
        "\n",
        "https://eurohpc-ju.europa.eu/ai-factories_en\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqMNyIA_Orvh"
      },
      "source": [
        "## Environment Variables\n",
        "we will need to use Environment Variables:\n",
        "- HF_TOKEN is you huggingface token, you may generate one on this url: https://huggingface.co/settings/tokens\n",
        "\n",
        "## [HPC] On Linux do:\n",
        "- `nano ~/.bashrc`\n",
        "- `export HF_TOKEN=\"...\"`\n",
        "- `source ~/.bashrc`\n",
        "- `echo $HF_TOKEN`\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcdbhQ1s_U1h"
      },
      "source": [
        "You may use Mistral-7B-Instruct-v0.3, Llama-3.2-1B-Instruct, ilsp/Llama-Krikri-8B-Instruct, or any other Transformers' compatible model.\n",
        "\n",
        "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3\n",
        "\n",
        "https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\n",
        "\n",
        "https://huggingface.co/ilsp/Llama-Krikri-8B-Instruct\n",
        "\n",
        "For some models, in order to be able to download them, you will need to accept the terms of use.\n",
        "\n",
        "You can check if you have been granted on:\n",
        "\n",
        "https://huggingface.co/settings/gated-repos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZxs35JyOI6Y"
      },
      "source": [
        "# Install dependencies\n",
        "- `pip install transformers`\n",
        "- `pip install accelerate`\n",
        "- `pip install --upgrade jinja2`\n",
        "- `pip install -U sentence-transformers`\n",
        "- `pip install pandas`\n",
        "- `pip install numpy`\n",
        "- `pip install scikit-learn`\n",
        "- `pip install --upgrade bitsandbytes`\n",
        "\n",
        "On google colab, you only need the last one *bitsandbytes*\n",
        "https://huggingface.co/docs/transformers/quantization/bitsandbytes?bnb=4-bit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHZrUloXhpAG"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFawOmNSQJny"
      },
      "source": [
        "# [HPC] Allocation of resources\n",
        "- `salloc -A pXYZ -p gpu --qos default -N 1 -t 08:00:00`\n",
        "- `salloc -A pXYZ -p gpu --qos default -N 1 -t 08:00:00 --gres=gpu:1`\n",
        "- Then, if you use vscode, do shift+enter on the python file. This will open a new terminal.\n",
        "- In the new terminal, do ctrl+z to stop the python script.\n",
        "- You need to do ssh on the allocated node. Get the node name from the previous terminal, after @ (`<username>@<node_name>`) and then do `ssh <node_name>` in the new terminal.\n",
        "- And then: `CUDA_VISIBLE_DEVICES=\"0,1,2,3\" python` or `CUDA_VISIBLE_DEVICES=\"0\" python`\n",
        "- Python should be launched now and you may run interactively your script."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ya6CiXEZQnbS"
      },
      "source": [
        "# [HPC] Usefull Commands\n",
        "- In the first terminal, you can do the following to monitor the resources:\n",
        "- `watch -n 1 \"top -bn1 | head -n 15 && nvidia-smi\"`\n",
        "- `du -sh .`\n",
        "- `watch nvidia-smi --query-compute-apps=pid,process_name,used_memory,gpu_name --format=csv`\n",
        "- `watch nvidia-smi --query-gpu=memory.total,memory.used,memory.free --format=csv`\n",
        "- `scancel <pid>`\n",
        "- `kill -9 <pid>`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XegLKNxhWtI"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBn7UR-2KRKD"
      },
      "source": [
        "https://huggingface.co/docs/transformers/index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dekk105Xef1m"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, pipeline, BitsAndBytesConfig\n",
        "import os\n",
        "import torch\n",
        "import time\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hXqF4y6fdfz"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "os.environ['HF_TOKEN'] = getpass.getpass(\"Enter the value for HF_TOKEN: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAVuzgKNl15o"
      },
      "outputs": [],
      "source": [
        "## HF_HOME is the directory where you want to save models' weights.\n",
        "## [HPC] use the project's directory and not the user's one, so as to have more space. export $HF_HOME as well.\n",
        "os.environ[\"HF_HOME\"] = \"/content/my_huggingface_cache\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euluA0sDhs7_"
      },
      "source": [
        "# The Transformers Library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eKFO83tha48"
      },
      "source": [
        "## Download Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3whaClpJfMWu"
      },
      "outputs": [],
      "source": [
        "my_model = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "# my_model = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "# my_model = \"ilsp/Llama-Krikri-8B-Instruct\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfX_L0I9faPg"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(my_model,\n",
        "                                          token=os.environ[\"HF_TOKEN\"],\n",
        "                                          cache_dir=os.environ[\"HF_HOME\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZvIpXkuf-e3"
      },
      "outputs": [],
      "source": [
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(   my_model,\n",
        "                                                token=os.environ[\"HF_TOKEN\"],\n",
        "                                                cache_dir=os.environ[\"HF_HOME\"],\n",
        "                                                device_map=\"auto\",\n",
        "                                                quantization_config=quantization_config,\n",
        "                                                torch_dtype=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTHqWqL8nIXc"
      },
      "source": [
        "## pad token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wRt6bvvgJsk"
      },
      "outputs": [],
      "source": [
        "# Depending on the model, the pad token might not be defined\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(\"Pad token was None, so it was set to eos token.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvIbkoAPnCWD"
      },
      "source": [
        "## Streamer for model.generate and pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEbL3_aYgVcz"
      },
      "outputs": [],
      "source": [
        "streamer = TextStreamer(tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjJZG-hahokL"
      },
      "source": [
        "## Messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VuFgHy6gY-D"
      },
      "outputs": [],
      "source": [
        "system_instructions = f\"You are a helpful assistant.\"\n",
        "my_messages = [{\"role\": \"system\", \"content\": system_instructions}]\n",
        "my_prompt = \"\"\"Explain linear regression using LaTeX.\n",
        "Use 'D' as the symbol for the dependent variable and 'I' as the symbol for the independent variable,\n",
        "in the regression equation.\"\"\"\n",
        "my_messages.append({\"role\": \"user\", \"content\": my_prompt})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GzZDHvkhzlH"
      },
      "source": [
        "## Streaming Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYnz0L3TnWvX"
      },
      "source": [
        "### Apply chat template to messages and return tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDvLZlCygb_s"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer.apply_chat_template(my_messages, return_tensors=\"pt\")\n",
        "print(type(inputs)) # <class 'torch.Tensor'>\n",
        "attention_mask = (inputs != tokenizer.pad_token_id).long()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTswWQMpnbRU"
      },
      "source": [
        "### Move inputs to GPU if available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPlVQFnfnabR"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.device_count()>0:\n",
        "    inputs = inputs.to(\"cuda\")\n",
        "    attention_mask = attention_mask.to(\"cuda\")\n",
        "    print(\"Inputs and Attention Mask transfered to CUDA\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcaH0TNUgoM1"
      },
      "outputs": [],
      "source": [
        "t1 = time.time()\n",
        "MAXIMUM_TOKENS = 512\n",
        "outputs = model.generate(inputs,\n",
        "                         streamer=streamer,\n",
        "                         pad_token_id=tokenizer.eos_token_id,\n",
        "                         attention_mask=attention_mask,\n",
        "                         max_new_tokens=MAXIMUM_TOKENS)\n",
        "t2 = time.time()\n",
        "print(type(outputs)) # <class 'torch.Tensor'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bkfh-CY3nvzL"
      },
      "source": [
        "### Clean the sesponse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vruyOo26gspc"
      },
      "outputs": [],
      "source": [
        "# To ommit <|begin_of_text|><|start_header_id|>system<|end_header_id|> we use:\n",
        "generated_text = tokenizer.decode(outputs[0],\n",
        "                                  skip_special_tokens=True,\n",
        "                                  clean_up_tokenization_spaces=True)\n",
        "print(f\"{generated_text}\\n\\n{(t2-t1)/60:.2f} minutes\")\n",
        "print(type(generated_text)) # <class 'str'>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "301kyfgnhIrO"
      },
      "outputs": [],
      "source": [
        "# To omit the system message we use:\n",
        "\n",
        "# For Llama\n",
        "# cleaned_text = re.sub(r\"^.*?assistant\\n\\n\", \"\", generated_text, flags=re.DOTALL)\n",
        "# print(cleaned_text + \"\\n\\n\" + f\"{(t2-t1)/60:.2f} minutes\")\n",
        "\n",
        "# For Mistral\n",
        "generated_text.split(my_prompt)[1][1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6srRs8Gvh4m4"
      },
      "source": [
        "## Inference (without steaming)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZ1eH4PahLal"
      },
      "outputs": [],
      "source": [
        "t1 = time.time()\n",
        "MAXIMUM_TOKENS = 128\n",
        "outputs = model.generate(inputs,\n",
        "                         pad_token_id=tokenizer.eos_token_id,\n",
        "                         attention_mask=attention_mask,\n",
        "                         max_new_tokens=MAXIMUM_TOKENS)\n",
        "t2 = time.time()\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(generated_text + \"\\n\\n\" + f\"{(t2-t1)/60:.2f} minutes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp3KXwWzh8-z"
      },
      "source": [
        "## The Pipeline object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4S4cv10ehOky"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline(\"text-generation\",\n",
        "                model=model,\n",
        "                tokenizer=tokenizer,\n",
        "                device_map=\"auto\")\n",
        "t1 = time.time()\n",
        "MAXIMUM_TOKENS = 128\n",
        "outputs = pipe(my_messages,\n",
        "               max_new_tokens=MAXIMUM_TOKENS,\n",
        "               pad_token_id=pipe.tokenizer.eos_token_id,\n",
        "               streamer=streamer)\n",
        "t2 = time.time()\n",
        "\n",
        "# In pipeline outputs (not in model.generate) we have the \"generated_text\" attribute:\n",
        "print(outputs[0][\"generated_text\"][-1]['content'] + \"\\n\\n\" + f\"{(t2-t1)/60:.2f} minutes\")\n",
        "# [{'generated_text': [{'role': 'system', 'content': 'You are a helful assistant.'},\n",
        "#                       {'role': 'user', 'content': \"...\n",
        "type(outputs) # <class 'list'>\n",
        "type(outputs[0]) # <class 'dict'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaULB1B1iAVF"
      },
      "source": [
        "# Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGJsMrqFiH5P"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpcfmiZXiBpf"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch\n",
        "import os\n",
        "if torch.cuda.device_count()>0:\n",
        "    my_device = \"cuda\"\n",
        "    print(f\"You have {torch.cuda.device_count()} GPUs available.\")\n",
        "else:\n",
        "    my_device = \"cpu\"\n",
        "    print(\"You have no GPUs available. Running on CPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKLpQL7niN3z"
      },
      "source": [
        "## The SentenceTransformer object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttfZb9iYiKcI"
      },
      "outputs": [],
      "source": [
        "embeddings_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2',\n",
        "                                       token=os.environ[\"HF_TOKEN\"],\n",
        "                                       cache_folder=os.environ[\"HF_HOME\"],\n",
        "                                       device=my_device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSDdVdidva_2"
      },
      "source": [
        "## Function to visualizing the similarity matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XKOzzvGvWWm"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def visualize_similarity_matrix(similarity_matrix, items_labels, mat_size=5):\n",
        "    for i in range(similarity_matrix.shape[0]):\n",
        "        similarity_matrix[i,i] = 0\n",
        "    plt.figure(figsize=(mat_size, mat_size))\n",
        "    plt.imshow(similarity_matrix, interpolation='nearest', cmap='viridis')\n",
        "    plt.colorbar(label=\"Cosine Similarity\")\n",
        "    plt.xticks(ticks=np.arange(len(items_labels)), labels=items_labels, rotation=90, fontsize=8)\n",
        "    plt.yticks(ticks=np.arange(len(items_labels)), labels=items_labels, fontsize=8)\n",
        "    plt.title(\"Cosine Similarity Matrix\", fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_9_SD2OimHP"
      },
      "source": [
        "## Test Embeddings - unrelated words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3luGfI7iSRM"
      },
      "outputs": [],
      "source": [
        "word_list = [\"reciprocal\", \"obfuscate\", \"hyperbolic\", \"tensor\"]\n",
        "word_embeddings = embeddings_model.encode(word_list)\n",
        "cosine_similarities = cosine_similarity(word_embeddings)\n",
        "print(\"Cosine Similarity Matrix:\")\n",
        "print(cosine_similarities)\n",
        "visualize_similarity_matrix(cosine_similarities, word_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alZvEv3Biv5L"
      },
      "source": [
        "## Test Embeddings - related words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6M2B2P9qiqVu"
      },
      "outputs": [],
      "source": [
        "word_list = [\"book\", \"publication\", \"article\"]\n",
        "word_embeddings = embeddings_model.encode(word_list)\n",
        "cosine_similarities = cosine_similarity(word_embeddings)\n",
        "print(\"Cosine Similarity Matrix:\")\n",
        "print(cosine_similarities)\n",
        "visualize_similarity_matrix(cosine_similarities, word_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ethNB13Ri2pv"
      },
      "source": [
        "## Calculate normalized mean values of embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8701DcBaivhF"
      },
      "outputs": [],
      "source": [
        "mean_embeddings = np.mean(np.abs(word_embeddings), axis=1)\n",
        "print(\"Normalized Mean values of embeddings:\", mean_embeddings)\n",
        "std_embeddings = np.std(word_embeddings, axis=1)\n",
        "print(\"Standard Deviation of embeddings:\", std_embeddings)\n",
        "norm_embeddings = np.linalg.norm(word_embeddings, axis=1)\n",
        "print(\"Norm of embeddings:\", norm_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv8MSkyTi6jJ"
      },
      "source": [
        "## Generate random vectors with the same mean and std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKHd_mEGi3m0"
      },
      "outputs": [],
      "source": [
        "random_vectors = np.random.normal(loc=np.mean(word_embeddings),\n",
        "                                  scale=np.std(word_embeddings),\n",
        "                                  size=word_embeddings.shape)\n",
        "mean_random_vectors = np.mean(np.abs(random_vectors), axis=1)\n",
        "print(\"Normalized Mean values of random vectors:\", mean_random_vectors)\n",
        "std_random_vectors = np.std(random_vectors, axis=1)\n",
        "print(\"Standard Deviation of random vectors:\", std_random_vectors)\n",
        "norm_random_vectors = np.linalg.norm(random_vectors, axis=1)\n",
        "print(\"Norm of random vectors:\", norm_random_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3RsT8FUTEHO"
      },
      "outputs": [],
      "source": [
        "print(\"Cosine Similarity Matrix random vectors:\")\n",
        "cosine_similarities = cosine_similarity(random_vectors)\n",
        "print(cosine_similarities)\n",
        "visualize_similarity_matrix(cosine_similarities, [\"Random Vector 1\", \"Random Vector 2\", \"Random Vector 3\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDJOes3HruF9"
      },
      "source": [
        "## car ~ vehicle + motorcycle - bike"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxoOEqnArrvt"
      },
      "outputs": [],
      "source": [
        "sentences = [\"car\", \"vehicle\", \"motorcycle\", \"bike\"]\n",
        "embeddings = embeddings_model.encode(sentences)\n",
        "print(cosine_similarity(embeddings[0].reshape(1, -1), (embeddings[1] + embeddings[2] - embeddings[3]).reshape(1, -1))[0, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_ooMezvr1-k"
      },
      "source": [
        "## Greece ~ Athens + Italy - Rome"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaFEGFx0rypQ"
      },
      "outputs": [],
      "source": [
        "sentences = [\"Greece\", \"Athens\", \"Italy\", \"Rome\"]\n",
        "embeddings = embeddings_model.encode(sentences)\n",
        "print(cosine_similarity((embeddings[0]).reshape(1, -1), (embeddings[1]+embeddings[2]-embeddings[3]).reshape(1, -1))[0, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8TzaG8nTH7m"
      },
      "source": [
        "So embeddings work!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufSNtaO5r7UY"
      },
      "source": [
        "## Sentence embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ANZU78xr6Ao"
      },
      "outputs": [],
      "source": [
        "my_sentences = [\n",
        "    # Interrelated sentences - group 1\n",
        "    \"The data is preprocessed to remove noise and outliers.\",\n",
        "    \"Noise and outliers are eliminated during data preprocessing.\",\n",
        "    \"Preprocessing cleans the data by filtering out noise and irregularities.\",\n",
        "\n",
        "    # Interrelated sentences - group 2\n",
        "    \"Paris is the capital of France.\",\n",
        "    \"Athens is the capital of Greece.\",\n",
        "    \"Rome is the capital of Italy.\"\n",
        "]\n",
        "my_embeddings = embeddings_model.encode(my_sentences)\n",
        "similarity_matrix = cosine_similarity(my_embeddings)\n",
        "print(similarity_matrix)\n",
        "visualize_similarity_matrix(similarity_matrix, my_sentences, mat_size=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbb60su0jE7d"
      },
      "source": [
        "# Retrieval Augmented Generation (RAG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3kyBQ_Zi7bj"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMl8vUL6jIic"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.device_count()>0:\n",
        "    my_device = \"cuda\"\n",
        "    print(f\"You have {torch.cuda.device_count()} GPUs available.\")\n",
        "else:\n",
        "    my_device = \"cpu\"\n",
        "    print(\"You have no GPUs available. Running on CPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15wz-R2Jjk0b"
      },
      "source": [
        "## Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aI_jbSMjKvN"
      },
      "outputs": [],
      "source": [
        "embeddings_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', token=os.environ[\"HF_TOKEN\"],\n",
        "                                       cache_folder=os.environ[\"HF_HOME\"], device=my_device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzINfM46jozb"
      },
      "source": [
        "## Text for retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSXW6j09jOOb"
      },
      "outputs": [],
      "source": [
        "my_text = \"\"\"\n",
        "This notebook demonstrates the use of large language models for generating text, embeddings and Retrieval Augmented Generation (RAG).\n",
        "It begins by setting up the model and tokenizer using the Hugging Face Transformers library, ensuring that the pad token is correctly defined.\n",
        "The notebook then illustrates how to generate text using the model in both streaming and non-streaming modes.\n",
        "It applies a chat template to user messages, moves inputs to a GPU if available, and generates outputs with a specified maximum number of tokens.\n",
        "The generated text is cleaned to remove system messages, and the time taken for generation is displayed.\n",
        "In addition to text generation, the notebook explores embeddings using the SentenceTransformer library.\n",
        "It encodes words and sentences to compute cosine similarity matrices, which are visualized to show relationships between different words and sentences.\n",
        "The notebook also demonstrates the concept of RAG by encoding a user's question and sorting sentences based on their similarity to the question.\n",
        "This approach helps in retrieving relevant information from a text corpus.\n",
        "Finally, the notebook sets up a pipeline for generating responses to user queries, showcasing the integration of text generation and retrieval techniques.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxJfHjKqjaDc"
      },
      "outputs": [],
      "source": [
        "my_sentences = my_text.split('\\n')\n",
        "my_sentences = [sent.strip() for sent in my_sentences if sent]\n",
        "my_embeddings = embeddings_model.encode(my_sentences)\n",
        "print(my_embeddings.shape)\n",
        "\n",
        "np.savetxt('sentence_embeddings.txt', my_embeddings, delimiter=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGQYJgWejxfj"
      },
      "source": [
        "## Encode user's question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XL1kl06Jjedc"
      },
      "outputs": [],
      "source": [
        "my_question = \"What is this notebook about?\"\n",
        "my_question_embedding = embeddings_model.encode([my_question])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQKIbgG3kIg0"
      },
      "source": [
        "## Sort sentences based on the similarity to the question embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhIrKbimkEKM"
      },
      "outputs": [],
      "source": [
        "similarity_to_question = cosine_similarity(my_question_embedding, my_embeddings).flatten()\n",
        "sorted_indices = similarity_to_question.argsort()[::-1]  # Sort in descending order\n",
        "print(sorted_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w11jHnI0kMaP"
      },
      "source": [
        "## Get sorted sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxOoVK4lkJ_9"
      },
      "outputs": [],
      "source": [
        "sorted_sentences = [my_sentences[i] for i in sorted_indices]\n",
        "print(\"Sorted sentences based on cosine similarity to the question:\")\n",
        "for i, sentence in enumerate(sorted_sentences):\n",
        "    print(\"-\"*100)\n",
        "    print(f\"Sentence {i+1}, similarity: {similarity_to_question[sorted_indices[i]]:.2f}\")\n",
        "    print(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfwdzDXOkeJb"
      },
      "source": [
        "## Setup messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DOEyRaukNVj"
      },
      "outputs": [],
      "source": [
        "nof_keep_sentences = 3\n",
        "system_instructions = f\"You are a helful assistant.\"\n",
        "my_messages = [{\"role\": \"system\", \"content\": system_instructions}]\n",
        "my_prompt = f\"Use the following sentences:\"\n",
        "for sentence in sorted_sentences[:nof_keep_sentences]:\n",
        "    my_prompt += f\"\\n{sentence}\"\n",
        "my_prompt += f\"\\n\\nAnswer the question:\\n\\n'{my_question}'\"\n",
        "my_messages.append({\"role\": \"user\", \"content\": my_prompt})\n",
        "my_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFaZA4MmkkqZ"
      },
      "source": [
        "## Answer the question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2obTGEmqkfEp"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, TextStreamer\n",
        "import os\n",
        "my_model = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "# my_model = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "# my_model = \"ilsp/Llama-Krikri-8B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(my_model,\n",
        "                                          token=os.environ[\"HF_TOKEN\"],\n",
        "                                          cache_dir=os.environ[\"HF_HOME\"])\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(   my_model,\n",
        "                                                token=os.environ[\"HF_TOKEN\"],\n",
        "                                                cache_dir=os.environ[\"HF_HOME\"],\n",
        "                                                device_map=\"auto\",\n",
        "                                                quantization_config=quantization_config,\n",
        "                                                torch_dtype=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEilz0-Xkzut"
      },
      "outputs": [],
      "source": [
        "# Depending on the model, the pad token might not be defined\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(\"Pad token was None, so it was set to eos token.\")\n",
        "\n",
        "streamer = TextStreamer(tokenizer)\n",
        "\n",
        "pipe = pipeline(\"text-generation\",\n",
        "                model=model,\n",
        "                tokenizer=tokenizer,\n",
        "                device_map=\"auto\")\n",
        "MAXIMUM_TOKENS = 128\n",
        "outputs = pipe(my_messages,\n",
        "               max_new_tokens=MAXIMUM_TOKENS,\n",
        "               pad_token_id=pipe.tokenizer.eos_token_id,\n",
        "               streamer=streamer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6jcPV3-o8pz"
      },
      "outputs": [],
      "source": [
        "my_output = outputs[0][\"generated_text\"][-1]['content']\n",
        "for i in range(0, len(my_output), 80):\n",
        "    print(my_output[i:i+80])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
